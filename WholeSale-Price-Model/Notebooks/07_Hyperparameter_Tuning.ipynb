{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T08:30:30.261013800Z",
     "start_time": "2026-02-16T08:30:29.681457400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, ParameterGrid\n",
    "from sklearn.model_selection import train_test_split\n",
    "from math import sqrt\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "\n",
    "print(\"All libraries imported successfully.\")"
   ],
   "id": "6a4fc83dd1bb9c83",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T08:30:30.314536900Z",
     "start_time": "2026-02-16T08:30:30.262014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define paths\n",
    "data_dir = \"../data/splits\"\n",
    "train_path = os.path.join(data_dir, \"train.csv\")\n",
    "val_path = os.path.join(data_dir, \"validation.csv\")\n",
    "test_path = os.path.join(data_dir, \"test.csv\")\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv(train_path)\n",
    "val_df = pd.read_csv(val_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "print(f\"Train set: {train_df.shape}\")\n",
    "print(f\"Validation set: {val_df.shape}\")\n",
    "print(f\"Test set: {test_df.shape}\")"
   ],
   "id": "267789286349807e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (3469, 40)\n",
      "Validation set: (743, 40)\n",
      "Test set: (744, 40)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T08:30:30.335569600Z",
     "start_time": "2026-02-16T08:30:30.315536400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Target column\n",
    "target_col = 'price'\n",
    "\n",
    "# Features: all columns except target\n",
    "feature_cols = [col for col in train_df.columns if col != target_col]\n",
    "\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df[target_col]\n",
    "\n",
    "X_val = val_df[feature_cols]\n",
    "y_val = val_df[target_col]\n",
    "\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df[target_col]\n",
    "\n",
    "print(\"Features and target separated.\")\n",
    "print(f\"Number of features: {len(feature_cols)}\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ],
   "id": "3668df6e84218b55",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features and target separated.\n",
      "Number of features: 39\n",
      "Training samples: 3469\n",
      "Validation samples: 743\n",
      "Test samples: 744\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T08:30:30.347089900Z",
     "start_time": "2026-02-16T08:30:30.335569600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_model(model, X_train, y_train, X_val, y_val, X_test, y_test, model_name):\n",
    "    \"\"\"Calculate metrics on train, validation, and test sets.\"\"\"\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    val_r2 = r2_score(y_val, y_val_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "    metrics = {\n",
    "        'Model': model_name,\n",
    "        'Train RMSE': train_rmse,\n",
    "        'Validation RMSE': val_rmse,\n",
    "        'Test RMSE': test_rmse,\n",
    "        'Train MAE': train_mae,\n",
    "        'Validation MAE': val_mae,\n",
    "        'Test MAE': test_mae,\n",
    "        'Train R2': train_r2,\n",
    "        'Validation R2': val_r2,\n",
    "        'Test R2': test_r2\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{model_name} Performance:\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"  Train RMSE: {train_rmse:.4f} | Validation RMSE: {val_rmse:.4f} | Test RMSE: {test_rmse:.4f}\")\n",
    "    print(f\"  Train MAE : {train_mae:.4f} | Validation MAE : {val_mae:.4f} | Test MAE : {test_mae:.4f}\")\n",
    "    print(f\"  Train R2  : {train_r2:.4f} | Validation R2  : {val_r2:.4f} | Test R2  : {test_r2:.4f}\")\n",
    "\n",
    "    overfit_gap = val_rmse - train_rmse\n",
    "    test_gap = test_rmse - train_rmse\n",
    "    print(f\"  Overfit Gap (Val-Train): {overfit_gap:.4f}\")\n",
    "    print(f\"  Test Gap (Test-Train): {test_gap:.4f}\")\n",
    "\n",
    "    return metrics, y_train_pred, y_val_pred, y_test_pred"
   ],
   "id": "bd1b60ab0b379145",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T08:30:48.805339200Z",
     "start_time": "2026-02-16T08:30:30.349087800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"=\"*60)\n",
    "print(\"OPTUNA TUNING XGBOOST (Validation-based, Strong Regularization)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import optuna\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def xgb_objective(trial):\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 4),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.01, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 0.7),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 0.7),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 5, 50, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 5, 50, log=True),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 10, 30),\n",
    "        'gamma': trial.suggest_float('gamma', 0.5, 2.0),\n",
    "        'n_estimators': 500,\n",
    "        'random_state': 42,\n",
    "        'objective': 'reg:squarederror',\n",
    "        'early_stopping_rounds': 10\n",
    "    }\n",
    "\n",
    "    model = xgb.XGBRegressor(**params)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "    pred = model.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, pred))\n",
    "    return rmse\n",
    "\n",
    "study_xgb = optuna.create_study(direction='minimize')\n",
    "study_xgb.optimize(xgb_objective, n_trials=30, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nBest XGBoost parameters: {study_xgb.best_params}\")\n",
    "print(f\"Best validation RMSE: {study_xgb.best_value:.4f}\")\n",
    "\n",
    "best_xgb_params = study_xgb.best_params.copy()\n",
    "best_xgb_params['n_estimators'] = 500\n",
    "best_xgb_params['random_state'] = 42\n",
    "best_xgb_params['objective'] = 'reg:squarederror'\n",
    "best_xgb_params['early_stopping_rounds'] = 10\n",
    "\n",
    "xgb_tuned = xgb.XGBRegressor(**best_xgb_params)\n",
    "xgb_tuned.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_train, y_train), (X_val, y_val)],   # both sets for learning curves\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "xgb_metrics, xgb_train_pred, xgb_val_pred, xgb_test_pred = evaluate_model(\n",
    "    xgb_tuned, X_train, y_train, X_val, y_val, X_test, y_test, \"XGBoost (Tuned)\"\n",
    ")\n",
    "\n",
    "models = {'XGBoost (Tuned)': xgb_tuned}"
   ],
   "id": "7f6357c135daa9dc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2026-02-16 14:00:30,435]\u001B[0m A new study created in memory with name: no-name-cdd56481-3698-4e60-8216-a36430a70315\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "OPTUNA TUNING XGBOOST (Validation-based, Strong Regularization)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b641babf02ee4c1b89e91ffa7a948499"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2026-02-16 14:00:31,899]\u001B[0m Trial 0 finished with value: 79.95215155651124 and parameters: {'max_depth': 4, 'learning_rate': 0.0016563509900164785, 'subsample': 0.5889288749460558, 'colsample_bytree': 0.69690631700422, 'reg_alpha': 40.598817491701425, 'reg_lambda': 43.360023285871684, 'min_child_weight': 21, 'gamma': 1.9058695927775855}. Best is trial 0 with value: 79.95215155651124.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:00:32,412]\u001B[0m Trial 1 finished with value: 59.61843439681585 and parameters: {'max_depth': 3, 'learning_rate': 0.008063477600199888, 'subsample': 0.5953526321097183, 'colsample_bytree': 0.6527958084188059, 'reg_alpha': 36.83119956165535, 'reg_lambda': 7.63237852888659, 'min_child_weight': 23, 'gamma': 1.2653139086606098}. Best is trial 1 with value: 59.61843439681585.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:00:33,123]\u001B[0m Trial 2 finished with value: 62.341993944060874 and parameters: {'max_depth': 4, 'learning_rate': 0.005377697588692437, 'subsample': 0.5320592697250028, 'colsample_bytree': 0.6571250463544638, 'reg_alpha': 13.606731143729885, 'reg_lambda': 27.066440385036568, 'min_child_weight': 29, 'gamma': 1.127553894357946}. Best is trial 1 with value: 59.61843439681585.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:00:33,958]\u001B[0m Trial 3 finished with value: 78.38833467457683 and parameters: {'max_depth': 4, 'learning_rate': 0.0014800728379999063, 'subsample': 0.5015390212986548, 'colsample_bytree': 0.6991887940811651, 'reg_alpha': 20.089640112737527, 'reg_lambda': 8.073072218866864, 'min_child_weight': 21, 'gamma': 1.968467735443082}. Best is trial 1 with value: 59.61843439681585.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:00:34,481]\u001B[0m Trial 4 finished with value: 81.95572653842932 and parameters: {'max_depth': 3, 'learning_rate': 0.0012635913478651224, 'subsample': 0.5194765129858773, 'colsample_bytree': 0.6902602811099936, 'reg_alpha': 13.470021177305636, 'reg_lambda': 5.879744751525089, 'min_child_weight': 11, 'gamma': 1.270248147873842}. Best is trial 1 with value: 59.61843439681585.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:00:35,020]\u001B[0m Trial 5 finished with value: 64.62611293176573 and parameters: {'max_depth': 3, 'learning_rate': 0.005968599674466633, 'subsample': 0.5142953253615958, 'colsample_bytree': 0.6037500339159578, 'reg_alpha': 20.004204222222192, 'reg_lambda': 34.328510781190474, 'min_child_weight': 14, 'gamma': 0.7203905801105656}. Best is trial 1 with value: 59.61843439681585.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:00:35,524]\u001B[0m Trial 6 finished with value: 61.022876103658575 and parameters: {'max_depth': 3, 'learning_rate': 0.00764121379091377, 'subsample': 0.5958898529165666, 'colsample_bytree': 0.546669015519345, 'reg_alpha': 26.337486393326156, 'reg_lambda': 12.285202657588918, 'min_child_weight': 22, 'gamma': 1.3970474267152906}. Best is trial 1 with value: 59.61843439681585.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:00:36,104]\u001B[0m Trial 7 finished with value: 56.94729346244477 and parameters: {'max_depth': 4, 'learning_rate': 0.009096392234795591, 'subsample': 0.5488391408816163, 'colsample_bytree': 0.5583559217887704, 'reg_alpha': 27.40828088694157, 'reg_lambda': 16.03854111480698, 'min_child_weight': 19, 'gamma': 0.6641910511987575}. Best is trial 7 with value: 56.94729346244477.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:00:36,605]\u001B[0m Trial 8 finished with value: 71.27946543375222 and parameters: {'max_depth': 3, 'learning_rate': 0.003345622136410864, 'subsample': 0.5641854388841296, 'colsample_bytree': 0.5002307811532923, 'reg_alpha': 9.242156480481912, 'reg_lambda': 6.162344169251678, 'min_child_weight': 17, 'gamma': 0.880237978529948}. Best is trial 7 with value: 56.94729346244477.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:00:37,029]\u001B[0m Trial 9 finished with value: 66.20656299994062 and parameters: {'max_depth': 2, 'learning_rate': 0.0072233149870213125, 'subsample': 0.5305045218482949, 'colsample_bytree': 0.6337283247309431, 'reg_alpha': 10.310445897253771, 'reg_lambda': 14.289669795201243, 'min_child_weight': 17, 'gamma': 1.910141850985671}. Best is trial 7 with value: 56.94729346244477.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:00:37,441]\u001B[0m Trial 10 finished with value: 74.755248149691 and parameters: {'max_depth': 2, 'learning_rate': 0.003464676263409555, 'subsample': 0.6722812914276888, 'colsample_bytree': 0.5548855038370005, 'reg_alpha': 5.222415538036576, 'reg_lambda': 21.97463019766149, 'min_child_weight': 27, 'gamma': 0.5449425306149236}. Best is trial 7 with value: 56.94729346244477.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:00:38,044]\u001B[0m Trial 11 finished with value: 55.65420794679633 and parameters: {'max_depth': 4, 'learning_rate': 0.009994504340957921, 'subsample': 0.6414802639146088, 'colsample_bytree': 0.5765343181457007, 'reg_alpha': 48.29076668350043, 'reg_lambda': 10.301351431335906, 'min_child_weight': 25, 'gamma': 1.4441364701618098}. Best is trial 11 with value: 55.65420794679633.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:00:38,620]\u001B[0m Trial 12 finished with value: 55.76007081646723 and parameters: {'max_depth': 4, 'learning_rate': 0.00967701030658552, 'subsample': 0.6526919749517051, 'colsample_bytree': 0.5684877286492294, 'reg_alpha': 48.08415617950703, 'reg_lambda': 11.17015931366186, 'min_child_weight': 25, 'gamma': 1.6050296369142725}. Best is trial 11 with value: 55.65420794679633.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:00:39,191]\u001B[0m Trial 13 finished with value: 63.92200030967295 and parameters: {'max_depth': 4, 'learning_rate': 0.0042800925798686645, 'subsample': 0.6531479505031401, 'colsample_bytree': 0.5900646385710918, 'reg_alpha': 49.16259774227069, 'reg_lambda': 10.575608002911231, 'min_child_weight': 25, 'gamma': 1.6101213208305156}. Best is trial 11 with value: 55.65420794679633.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:00:39,802]\u001B[0m Trial 14 finished with value: 77.93286658935583 and parameters: {'max_depth': 4, 'learning_rate': 0.0020529528004971987, 'subsample': 0.6388389348134345, 'colsample_bytree': 0.520311188093048, 'reg_alpha': 32.225590418065195, 'reg_lambda': 19.212004677892555, 'min_child_weight': 26, 'gamma': 1.5957770830943152}. Best is trial 11 with value: 55.65420794679633.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:00:40,378]\u001B[0m Trial 15 finished with value: 55.79249316125679 and parameters: {'max_depth': 4, 'learning_rate': 0.009718214905737158, 'subsample': 0.6371630666398758, 'colsample_bytree': 0.587387081210866, 'reg_alpha': 43.94228222757851, 'reg_lambda': 9.188366575683707, 'min_child_weight': 30, 'gamma': 1.6356878020200236}. Best is trial 11 with value: 55.65420794679633.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:00:40,972]\u001B[0m Trial 16 finished with value: 62.29453654095807 and parameters: {'max_depth': 4, 'learning_rate': 0.0051304364331164354, 'subsample': 0.6941422098932595, 'colsample_bytree': 0.563759161952291, 'reg_alpha': 49.37133074686515, 'reg_lambda': 11.551764864114329, 'min_child_weight': 27, 'gamma': 1.0385123935522114}. Best is trial 11 with value: 55.65420794679633.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:00:41,392]\u001B[0m Trial 17 finished with value: 78.02654934313408 and parameters: {'max_depth': 2, 'learning_rate': 0.002405044041012113, 'subsample': 0.6259532164910874, 'colsample_bytree': 0.6253745306211512, 'reg_alpha': 30.42197849524515, 'reg_lambda': 16.47643481935791, 'min_child_weight': 24, 'gamma': 1.4442902278101788}. Best is trial 11 with value: 55.65420794679633.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:00:41,885]\u001B[0m Trial 18 finished with value: 62.368555019701624 and parameters: {'max_depth': 3, 'learning_rate': 0.006491603241421305, 'subsample': 0.6698540219622012, 'colsample_bytree': 0.5345844586074215, 'reg_alpha': 22.77832163551853, 'reg_lambda': 7.193653582417258, 'min_child_weight': 19, 'gamma': 1.733807894531914}. Best is trial 11 with value: 55.65420794679633.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:00:42,460]\u001B[0m Trial 19 finished with value: 63.15205579454145 and parameters: {'max_depth': 4, 'learning_rate': 0.004717149830965643, 'subsample': 0.6157103634225857, 'colsample_bytree': 0.5827087132121814, 'reg_alpha': 33.6935868693423, 'reg_lambda': 10.449359612350369, 'min_child_weight': 28, 'gamma': 1.4379462185562395}. Best is trial 11 with value: 55.65420794679633.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:00:42,948]\u001B[0m Trial 20 finished with value: 59.200079195491675 and parameters: {'max_depth': 3, 'learning_rate': 0.009872234551127481, 'subsample': 0.6962360183622269, 'colsample_bytree': 0.6150440350188414, 'reg_alpha': 16.88866270587961, 'reg_lambda': 24.45158250747426, 'min_child_weight': 24, 'gamma': 1.8031182341672298}. Best is trial 11 with value: 55.65420794679633.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:00:43,532]\u001B[0m Trial 21 finished with value: 55.73583248923026 and parameters: {'max_depth': 4, 'learning_rate': 0.00966839602714733, 'subsample': 0.6448890612980712, 'colsample_bytree': 0.5731697062956284, 'reg_alpha': 40.45332980503088, 'reg_lambda': 8.826264441625392, 'min_child_weight': 30, 'gamma': 1.6129278733832988}. Best is trial 11 with value: 55.65420794679633.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:00:44,140]\u001B[0m Trial 22 finished with value: 57.95270935739154 and parameters: {'max_depth': 4, 'learning_rate': 0.007512031974158768, 'subsample': 0.661470445830564, 'colsample_bytree': 0.5723228344893043, 'reg_alpha': 39.42185667524467, 'reg_lambda': 13.85197676174548, 'min_child_weight': 30, 'gamma': 1.5354257996829412}. Best is trial 11 with value: 55.65420794679633.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:00:44,735]\u001B[0m Trial 23 finished with value: 56.903111478014274 and parameters: {'max_depth': 4, 'learning_rate': 0.008398266067707173, 'subsample': 0.6141847777406475, 'colsample_bytree': 0.5312815561560886, 'reg_alpha': 41.221052661343435, 'reg_lambda': 5.306323813488414, 'min_child_weight': 26, 'gamma': 1.764302012235183}. Best is trial 11 with value: 55.65420794679633.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:00:45,310]\u001B[0m Trial 24 finished with value: 85.17729016470838 and parameters: {'max_depth': 4, 'learning_rate': 0.0010175855204269237, 'subsample': 0.6493346521095779, 'colsample_bytree': 0.6040158707658605, 'reg_alpha': 49.67192641507179, 'reg_lambda': 8.777712504939974, 'min_child_weight': 29, 'gamma': 1.1388414879413982}. Best is trial 11 with value: 55.65420794679633.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:00:45,902]\u001B[0m Trial 25 finished with value: 59.28472859431641 and parameters: {'max_depth': 4, 'learning_rate': 0.0061278432233179275, 'subsample': 0.6811868369100528, 'colsample_bytree': 0.5724208907275032, 'reg_alpha': 26.801018555596784, 'reg_lambda': 6.86372035204565, 'min_child_weight': 25, 'gamma': 1.3730566801429949}. Best is trial 11 with value: 55.65420794679633.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:00:46,485]\u001B[0m Trial 26 finished with value: 72.41848282012919 and parameters: {'max_depth': 4, 'learning_rate': 0.002597386248471691, 'subsample': 0.6339721968621054, 'colsample_bytree': 0.5426157320099907, 'reg_alpha': 35.09681660562405, 'reg_lambda': 8.690250232524809, 'min_child_weight': 28, 'gamma': 1.5060232256510075}. Best is trial 11 with value: 55.65420794679633.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:00:47,065]\u001B[0m Trial 27 finished with value: 64.71032745436973 and parameters: {'max_depth': 4, 'learning_rate': 0.004298219749163468, 'subsample': 0.6128266641639952, 'colsample_bytree': 0.574775926339036, 'reg_alpha': 5.804404987503393, 'reg_lambda': 12.951927698455455, 'min_child_weight': 23, 'gamma': 1.704482661529051}. Best is trial 11 with value: 55.65420794679633.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:00:47,540]\u001B[0m Trial 28 finished with value: 59.20696577574219 and parameters: {'max_depth': 3, 'learning_rate': 0.009958322876683301, 'subsample': 0.5771597693369499, 'colsample_bytree': 0.5142663243801716, 'reg_alpha': 44.137926499135, 'reg_lambda': 9.948646416935404, 'min_child_weight': 27, 'gamma': 1.8469194822307609}. Best is trial 11 with value: 55.65420794679633.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:00:48,143]\u001B[0m Trial 29 finished with value: 61.35129486001537 and parameters: {'max_depth': 4, 'learning_rate': 0.006890927761425349, 'subsample': 0.6572456037231141, 'colsample_bytree': 0.594953643032634, 'reg_alpha': 38.48670381955472, 'reg_lambda': 47.78023011176888, 'min_child_weight': 21, 'gamma': 1.6583182336865447}. Best is trial 11 with value: 55.65420794679633.\u001B[0m\n",
      "\n",
      "Best XGBoost parameters: {'max_depth': 4, 'learning_rate': 0.009994504340957921, 'subsample': 0.6414802639146088, 'colsample_bytree': 0.5765343181457007, 'reg_alpha': 48.29076668350043, 'reg_lambda': 10.301351431335906, 'min_child_weight': 25, 'gamma': 1.4441364701618098}\n",
      "Best validation RMSE: 55.6542\n",
      "\n",
      "==================================================\n",
      "XGBoost (Tuned) Performance:\n",
      "==================================================\n",
      "  Train RMSE: 58.5816 | Validation RMSE: 55.6542 | Test RMSE: 52.2491\n",
      "  Train MAE : 34.7720 | Validation MAE : 36.2779 | Test MAE : 35.5702\n",
      "  Train R2  : 0.7033 | Validation R2  : 0.6874 | Test R2  : 0.7064\n",
      "  Overfit Gap (Val-Train): -2.9274\n",
      "  Test Gap (Test-Train): -6.3325\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T08:30:59.041763700Z",
     "start_time": "2026-02-16T08:30:48.864258100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OPTUNA TUNING LIGHTGBM (Validation-based, Strong Regularization)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def lgb_objective(trial):\n",
    "    params = {\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 10, 40),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 6),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.01, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 0.7),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 0.7),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1, 20, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1, 20, log=True),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 30, 100),\n",
    "        'n_estimators': 500,\n",
    "        'random_state': 42,\n",
    "        'verbosity': -1\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMRegressor(**params)\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_metric='rmse',\n",
    "        callbacks=[lgb.early_stopping(10)]\n",
    "    )\n",
    "    pred = model.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, pred))\n",
    "    return rmse\n",
    "\n",
    "study_lgb = optuna.create_study(direction='minimize')\n",
    "study_lgb.optimize(lgb_objective, n_trials=30, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nBest LightGBM parameters: {study_lgb.best_params}\")\n",
    "print(f\"Best validation RMSE: {study_lgb.best_value:.4f}\")\n",
    "\n",
    "best_lgb_params = study_lgb.best_params.copy()\n",
    "best_lgb_params['n_estimators'] = 500\n",
    "best_lgb_params['random_state'] = 42\n",
    "best_lgb_params['verbosity'] = -1\n",
    "\n",
    "lgb_tuned = lgb.LGBMRegressor(**best_lgb_params)\n",
    "lgb_tuned.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_train, y_train), (X_val, y_val)],   # both sets\n",
    "    eval_metric='rmse',\n",
    "    callbacks=[lgb.early_stopping(10)]\n",
    ")\n",
    "\n",
    "lgb_metrics, lgb_train_pred, lgb_val_pred, lgb_test_pred = evaluate_model(\n",
    "    lgb_tuned, X_train, y_train, X_val, y_val, X_test, y_test, \"LightGBM (Tuned)\"\n",
    ")\n",
    "\n",
    "models['LightGBM (Tuned)'] = lgb_tuned"
   ],
   "id": "78f97e2043a67fa",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2026-02-16 14:00:48,874]\u001B[0m A new study created in memory with name: no-name-b374f8f3-cb51-474a-9c66-0fc98084c1ac\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "OPTUNA TUNING LIGHTGBM (Validation-based, Strong Regularization)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "50814f6fe5b543daa91ee8da9e1c9a8d"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's rmse: 58.608\tvalid_0's l2: 3434.9\n",
      "\u001B[32m[I 2026-02-16 14:00:49,369]\u001B[0m Trial 0 finished with value: 58.60801629183911 and parameters: {'num_leaves': 27, 'max_depth': 6, 'learning_rate': 0.00529805716495862, 'subsample': 0.690809289323125, 'colsample_bytree': 0.5014558159768551, 'reg_alpha': 9.489371685115119, 'reg_lambda': 3.1624622588523383, 'min_child_samples': 58}. Best is trial 0 with value: 58.60801629183911.\u001B[0m\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's rmse: 69.8841\tvalid_0's l2: 4883.79\n",
      "\u001B[32m[I 2026-02-16 14:00:49,542]\u001B[0m Trial 1 finished with value: 69.88409486873272 and parameters: {'num_leaves': 13, 'max_depth': 3, 'learning_rate': 0.002927112722614055, 'subsample': 0.6413068605320908, 'colsample_bytree': 0.6468216987247428, 'reg_alpha': 3.3867840446247204, 'reg_lambda': 2.7315356658872467, 'min_child_samples': 46}. Best is trial 0 with value: 58.60801629183911.\u001B[0m\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's rmse: 62.3684\tvalid_0's l2: 3889.82\n",
      "\u001B[32m[I 2026-02-16 14:00:49,838]\u001B[0m Trial 2 finished with value: 62.36843458095443 and parameters: {'num_leaves': 18, 'max_depth': 6, 'learning_rate': 0.00394246130967622, 'subsample': 0.6555084981996915, 'colsample_bytree': 0.5840416806261945, 'reg_alpha': 5.846721696150891, 'reg_lambda': 5.589169836945854, 'min_child_samples': 64}. Best is trial 0 with value: 58.60801629183911.\u001B[0m\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's rmse: 65.7176\tvalid_0's l2: 4318.8\n",
      "\u001B[32m[I 2026-02-16 14:00:50,078]\u001B[0m Trial 3 finished with value: 65.71757617065033 and parameters: {'num_leaves': 39, 'max_depth': 4, 'learning_rate': 0.0034484175195378724, 'subsample': 0.6394038685924908, 'colsample_bytree': 0.6229450829194052, 'reg_alpha': 12.856073955234512, 'reg_lambda': 9.78976705273403, 'min_child_samples': 38}. Best is trial 0 with value: 58.60801629183911.\u001B[0m\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's rmse: 70.3412\tvalid_0's l2: 4947.89\n",
      "\u001B[32m[I 2026-02-16 14:00:50,245]\u001B[0m Trial 4 finished with value: 70.34121018928755 and parameters: {'num_leaves': 36, 'max_depth': 3, 'learning_rate': 0.002938332716355778, 'subsample': 0.5631892813381844, 'colsample_bytree': 0.6091969620484812, 'reg_alpha': 1.1518833430470534, 'reg_lambda': 4.487785900244732, 'min_child_samples': 81}. Best is trial 0 with value: 58.60801629183911.\u001B[0m\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's rmse: 76.3537\tvalid_0's l2: 5829.89\n",
      "\u001B[32m[I 2026-02-16 14:00:50,474]\u001B[0m Trial 5 finished with value: 76.35370360555748 and parameters: {'num_leaves': 12, 'max_depth': 4, 'learning_rate': 0.001626930302385954, 'subsample': 0.5531284824369335, 'colsample_bytree': 0.6867926939696977, 'reg_alpha': 1.4784419059575307, 'reg_lambda': 5.042403037464208, 'min_child_samples': 90}. Best is trial 0 with value: 58.60801629183911.\u001B[0m\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's rmse: 55.6083\tvalid_0's l2: 3092.28\n",
      "\u001B[32m[I 2026-02-16 14:00:50,696]\u001B[0m Trial 6 finished with value: 55.608281296816635 and parameters: {'num_leaves': 16, 'max_depth': 4, 'learning_rate': 0.0094979105618909, 'subsample': 0.548140905073576, 'colsample_bytree': 0.6435096635932187, 'reg_alpha': 3.70212132254711, 'reg_lambda': 15.539097509203586, 'min_child_samples': 66}. Best is trial 6 with value: 55.608281296816635.\u001B[0m\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's rmse: 56.8372\tvalid_0's l2: 3230.47\n",
      "\u001B[32m[I 2026-02-16 14:00:51,023]\u001B[0m Trial 7 finished with value: 56.837185297119476 and parameters: {'num_leaves': 31, 'max_depth': 6, 'learning_rate': 0.006502670049291113, 'subsample': 0.6894094983239267, 'colsample_bytree': 0.6325458427915058, 'reg_alpha': 5.500871974781027, 'reg_lambda': 1.3725496993296697, 'min_child_samples': 99}. Best is trial 6 with value: 55.608281296816635.\u001B[0m\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's rmse: 64.3325\tvalid_0's l2: 4138.68\n",
      "\u001B[32m[I 2026-02-16 14:00:51,179]\u001B[0m Trial 8 finished with value: 64.33253574422308 and parameters: {'num_leaves': 12, 'max_depth': 3, 'learning_rate': 0.005369741124331629, 'subsample': 0.5915865855523272, 'colsample_bytree': 0.515880270331983, 'reg_alpha': 1.1500999518637647, 'reg_lambda': 1.5949594562518747, 'min_child_samples': 94}. Best is trial 6 with value: 55.608281296816635.\u001B[0m\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's rmse: 57.3127\tvalid_0's l2: 3284.74\n",
      "\u001B[32m[I 2026-02-16 14:00:51,571]\u001B[0m Trial 9 finished with value: 57.312688854714494 and parameters: {'num_leaves': 31, 'max_depth': 6, 'learning_rate': 0.005337783979832834, 'subsample': 0.6988669043645926, 'colsample_bytree': 0.6521238603597509, 'reg_alpha': 12.167519811438106, 'reg_lambda': 7.2720972495594225, 'min_child_samples': 55}. Best is trial 6 with value: 55.608281296816635.\u001B[0m\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's rmse: 54.6579\tvalid_0's l2: 2987.49\n",
      "\u001B[32m[I 2026-02-16 14:00:51,842]\u001B[0m Trial 10 finished with value: 54.657908050147704 and parameters: {'num_leaves': 20, 'max_depth': 5, 'learning_rate': 0.009911284016117718, 'subsample': 0.5027092235706254, 'colsample_bytree': 0.5657580583683458, 'reg_alpha': 2.403060041815242, 'reg_lambda': 19.559156794032432, 'min_child_samples': 75}. Best is trial 10 with value: 54.657908050147704.\u001B[0m\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's rmse: 55.7709\tvalid_0's l2: 3110.4\n",
      "\u001B[32m[I 2026-02-16 14:00:52,122]\u001B[0m Trial 11 finished with value: 55.77091744763839 and parameters: {'num_leaves': 21, 'max_depth': 5, 'learning_rate': 0.00866762258529963, 'subsample': 0.5021650346349195, 'colsample_bytree': 0.564084402683649, 'reg_alpha': 2.95250897026245, 'reg_lambda': 18.722754025134417, 'min_child_samples': 76}. Best is trial 10 with value: 54.657908050147704.\u001B[0m\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's rmse: 55.3593\tvalid_0's l2: 3064.65\n",
      "\u001B[32m[I 2026-02-16 14:00:52,400]\u001B[0m Trial 12 finished with value: 55.359301505446915 and parameters: {'num_leaves': 19, 'max_depth': 5, 'learning_rate': 0.009364623648551949, 'subsample': 0.5086887811324609, 'colsample_bytree': 0.5470483065928436, 'reg_alpha': 2.2576642112011154, 'reg_lambda': 19.746459901552836, 'min_child_samples': 74}. Best is trial 10 with value: 54.657908050147704.\u001B[0m\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's rmse: 84.2527\tvalid_0's l2: 7098.51\n",
      "\u001B[32m[I 2026-02-16 14:00:52,680]\u001B[0m Trial 13 finished with value: 84.25265293768105 and parameters: {'num_leaves': 23, 'max_depth': 5, 'learning_rate': 0.001073499343016714, 'subsample': 0.5085631137779577, 'colsample_bytree': 0.5427121798311156, 'reg_alpha': 2.0785834840638198, 'reg_lambda': 11.956866651384832, 'min_child_samples': 76}. Best is trial 10 with value: 54.657908050147704.\u001B[0m\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's rmse: 56.4685\tvalid_0's l2: 3188.69\n",
      "\u001B[32m[I 2026-02-16 14:00:52,949]\u001B[0m Trial 14 finished with value: 56.46852758257371 and parameters: {'num_leaves': 18, 'max_depth': 5, 'learning_rate': 0.008254667205481752, 'subsample': 0.5271597165497979, 'colsample_bytree': 0.5451903311582494, 'reg_alpha': 2.10279172201241, 'reg_lambda': 19.9607319311114, 'min_child_samples': 82}. Best is trial 10 with value: 54.657908050147704.\u001B[0m\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's rmse: 74.5945\tvalid_0's l2: 5564.33\n",
      "\u001B[32m[I 2026-02-16 14:00:53,238]\u001B[0m Trial 15 finished with value: 74.5944663625764 and parameters: {'num_leaves': 26, 'max_depth': 5, 'learning_rate': 0.0019419286864746563, 'subsample': 0.598016070047796, 'colsample_bytree': 0.5829303992320657, 'reg_alpha': 2.116122259290619, 'reg_lambda': 10.319957378300632, 'min_child_samples': 69}. Best is trial 10 with value: 54.657908050147704.\u001B[0m\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's rmse: 57.6578\tvalid_0's l2: 3324.43\n",
      "\u001B[32m[I 2026-02-16 14:00:53,514]\u001B[0m Trial 16 finished with value: 57.65783635508737 and parameters: {'num_leaves': 20, 'max_depth': 5, 'learning_rate': 0.006852713680136289, 'subsample': 0.5281839402771175, 'colsample_bytree': 0.5332279434827608, 'reg_alpha': 7.192961104874755, 'reg_lambda': 8.539758492860814, 'min_child_samples': 88}. Best is trial 10 with value: 54.657908050147704.\u001B[0m\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's rmse: 55.8218\tvalid_0's l2: 3116.08\n",
      "\u001B[32m[I 2026-02-16 14:00:53,748]\u001B[0m Trial 17 finished with value: 55.82183157803554 and parameters: {'num_leaves': 16, 'max_depth': 4, 'learning_rate': 0.00973641738362348, 'subsample': 0.5713098819813559, 'colsample_bytree': 0.5655243547478936, 'reg_alpha': 18.51665407033753, 'reg_lambda': 13.64039729464753, 'min_child_samples': 54}. Best is trial 10 with value: 54.657908050147704.\u001B[0m\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's rmse: 55.3053\tvalid_0's l2: 3058.67\n",
      "\u001B[32m[I 2026-02-16 14:00:54,092]\u001B[0m Trial 18 finished with value: 55.30525945003129 and parameters: {'num_leaves': 24, 'max_depth': 5, 'learning_rate': 0.007113804234742951, 'subsample': 0.5320758278994937, 'colsample_bytree': 0.567443846369353, 'reg_alpha': 2.7737430774836165, 'reg_lambda': 6.705706803964969, 'min_child_samples': 30}. Best is trial 10 with value: 54.657908050147704.\u001B[0m\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's rmse: 62.248\tvalid_0's l2: 3874.82\n",
      "\u001B[32m[I 2026-02-16 14:00:54,353]\u001B[0m Trial 19 finished with value: 62.248035934789876 and parameters: {'num_leaves': 30, 'max_depth': 4, 'learning_rate': 0.0043774765209251484, 'subsample': 0.5842437265321556, 'colsample_bytree': 0.5969861034016646, 'reg_alpha': 4.482642378536995, 'reg_lambda': 2.4182507602203662, 'min_child_samples': 32}. Best is trial 10 with value: 54.657908050147704.\u001B[0m\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's rmse: 55.1448\tvalid_0's l2: 3040.95\n",
      "\u001B[32m[I 2026-02-16 14:00:54,716]\u001B[0m Trial 20 finished with value: 55.14480440836103 and parameters: {'num_leaves': 24, 'max_depth': 6, 'learning_rate': 0.006957112868602387, 'subsample': 0.5361988100344743, 'colsample_bytree': 0.571239522562611, 'reg_alpha': 1.4834294672263795, 'reg_lambda': 6.8628750740822175, 'min_child_samples': 45}. Best is trial 10 with value: 54.657908050147704.\u001B[0m\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's rmse: 54.4185\tvalid_0's l2: 2961.37\n",
      "\u001B[32m[I 2026-02-16 14:00:55,092]\u001B[0m Trial 21 finished with value: 54.418515826035446 and parameters: {'num_leaves': 24, 'max_depth': 6, 'learning_rate': 0.007192537849077025, 'subsample': 0.5304163936140222, 'colsample_bytree': 0.5680070752267884, 'reg_alpha': 1.5267321472784305, 'reg_lambda': 6.455384736927628, 'min_child_samples': 30}. Best is trial 21 with value: 54.418515826035446.\u001B[0m\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[499]\tvalid_0's rmse: 53.9514\tvalid_0's l2: 2910.75\n",
      "\u001B[32m[I 2026-02-16 14:00:55,495]\u001B[0m Trial 22 finished with value: 53.9513519486668 and parameters: {'num_leaves': 28, 'max_depth': 6, 'learning_rate': 0.0072244286868920725, 'subsample': 0.5306798563487414, 'colsample_bytree': 0.5864508719746131, 'reg_alpha': 1.435360627542417, 'reg_lambda': 3.750161793668603, 'min_child_samples': 43}. Best is trial 22 with value: 53.9513519486668.\u001B[0m\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's rmse: 55.037\tvalid_0's l2: 3029.07\n",
      "\u001B[32m[I 2026-02-16 14:00:55,911]\u001B[0m Trial 23 finished with value: 55.03696346469585 and parameters: {'num_leaves': 28, 'max_depth': 6, 'learning_rate': 0.006115056373361471, 'subsample': 0.5192153670860175, 'colsample_bytree': 0.6039503998036249, 'reg_alpha': 1.6319201553810503, 'reg_lambda': 3.5250808789123975, 'min_child_samples': 41}. Best is trial 22 with value: 53.9513519486668.\u001B[0m\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[499]\tvalid_0's rmse: 52.8984\tvalid_0's l2: 2798.25\n",
      "\u001B[32m[I 2026-02-16 14:00:56,342]\u001B[0m Trial 24 finished with value: 52.89844798640293 and parameters: {'num_leaves': 34, 'max_depth': 6, 'learning_rate': 0.0076987362624056635, 'subsample': 0.6142266152906882, 'colsample_bytree': 0.5879307840394242, 'reg_alpha': 1.7138760188984496, 'reg_lambda': 2.009786281066718, 'min_child_samples': 37}. Best is trial 24 with value: 52.89844798640293.\u001B[0m\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's rmse: 59.1676\tvalid_0's l2: 3500.81\n",
      "\u001B[32m[I 2026-02-16 14:00:56,812]\u001B[0m Trial 25 finished with value: 59.167647061358785 and parameters: {'num_leaves': 34, 'max_depth': 6, 'learning_rate': 0.004243620251034117, 'subsample': 0.616114419476995, 'colsample_bytree': 0.5879580117054745, 'reg_alpha': 1.6776701931441258, 'reg_lambda': 1.000412127990392, 'min_child_samples': 36}. Best is trial 24 with value: 52.89844798640293.\u001B[0m\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's rmse: 67.8691\tvalid_0's l2: 4606.21\n",
      "\u001B[32m[I 2026-02-16 14:00:57,250]\u001B[0m Trial 26 finished with value: 67.86908986335575 and parameters: {'num_leaves': 34, 'max_depth': 6, 'learning_rate': 0.002435372041237419, 'subsample': 0.613138949314537, 'colsample_bytree': 0.616858406903265, 'reg_alpha': 1.0183934805390433, 'reg_lambda': 2.120279084830775, 'min_child_samples': 43}. Best is trial 24 with value: 52.89844798640293.\u001B[0m\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's rmse: 53.3129\tvalid_0's l2: 2842.27\n",
      "\u001B[32m[I 2026-02-16 14:00:57,671]\u001B[0m Trial 27 finished with value: 53.312915632244476 and parameters: {'num_leaves': 39, 'max_depth': 6, 'learning_rate': 0.0077996649136477915, 'subsample': 0.5724200164780836, 'colsample_bytree': 0.6748018877372135, 'reg_alpha': 1.3307942998012212, 'reg_lambda': 3.5061911227119555, 'min_child_samples': 49}. Best is trial 24 with value: 52.89844798640293.\u001B[0m\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's rmse: 53.0989\tvalid_0's l2: 2819.5\n",
      "\u001B[32m[I 2026-02-16 14:00:58,106]\u001B[0m Trial 28 finished with value: 53.09893772939127 and parameters: {'num_leaves': 39, 'max_depth': 6, 'learning_rate': 0.007992409782639707, 'subsample': 0.5747483526669723, 'colsample_bytree': 0.6732163158301016, 'reg_alpha': 1.1970687482939526, 'reg_lambda': 3.770997151935153, 'min_child_samples': 49}. Best is trial 24 with value: 52.89844798640293.\u001B[0m\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[500]\tvalid_0's rmse: 57.1258\tvalid_0's l2: 3263.36\n",
      "\u001B[32m[I 2026-02-16 14:00:58,577]\u001B[0m Trial 29 finished with value: 57.12584618447302 and parameters: {'num_leaves': 40, 'max_depth': 6, 'learning_rate': 0.004859140145536187, 'subsample': 0.5771026780536822, 'colsample_bytree': 0.6961715900630061, 'reg_alpha': 1.2130570085768442, 'reg_lambda': 1.8564117738302386, 'min_child_samples': 50}. Best is trial 24 with value: 52.89844798640293.\u001B[0m\n",
      "\n",
      "Best LightGBM parameters: {'num_leaves': 34, 'max_depth': 6, 'learning_rate': 0.0076987362624056635, 'subsample': 0.6142266152906882, 'colsample_bytree': 0.5879307840394242, 'reg_alpha': 1.7138760188984496, 'reg_lambda': 2.009786281066718, 'min_child_samples': 37}\n",
      "Best validation RMSE: 52.8984\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[499]\ttraining's rmse: 52.5868\ttraining's l2: 2765.37\tvalid_1's rmse: 52.8984\tvalid_1's l2: 2798.25\n",
      "\n",
      "==================================================\n",
      "LightGBM (Tuned) Performance:\n",
      "==================================================\n",
      "  Train RMSE: 52.5868 | Validation RMSE: 52.8984 | Test RMSE: 49.6772\n",
      "  Train MAE : 30.3834 | Validation MAE : 34.5542 | Test MAE : 33.7795\n",
      "  Train R2  : 0.7609 | Validation R2  : 0.7176 | Test R2  : 0.7345\n",
      "  Overfit Gap (Val-Train): 0.3116\n",
      "  Test Gap (Test-Train): -2.9096\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T08:31:23.263094500Z",
     "start_time": "2026-02-16T08:30:59.092799400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OPTUNA TUNING CATBOOST (Validation-based, Strong Regularization)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def cb_objective(trial):\n",
    "    params = {\n",
    "        'depth': trial.suggest_int('depth', 3, 5),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.03, log=True),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 5, 20, log=True),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 1, 3),\n",
    "        'border_count': trial.suggest_int('border_count', 32, 64),\n",
    "        'iterations': 500,\n",
    "        'random_seed': 42,\n",
    "        'early_stopping_rounds': 10,\n",
    "        'verbose': False\n",
    "    }\n",
    "\n",
    "    model = cb.CatBoostRegressor(**params)\n",
    "    model.fit(X_train, y_train, eval_set=(X_val, y_val), verbose=False)\n",
    "    pred = model.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, pred))\n",
    "    return rmse\n",
    "\n",
    "study_cb = optuna.create_study(direction='minimize')\n",
    "study_cb.optimize(cb_objective, n_trials=30, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nBest CatBoost parameters: {study_cb.best_params}\")\n",
    "print(f\"Best validation RMSE: {study_cb.best_value:.4f}\")\n",
    "\n",
    "best_cb_params = study_cb.best_params.copy()\n",
    "best_cb_params['iterations'] = 500\n",
    "best_cb_params['random_seed'] = 42\n",
    "best_cb_params['early_stopping_rounds'] = 10\n",
    "best_cb_params['verbose'] = False\n",
    "\n",
    "cb_tuned = cb.CatBoostRegressor(**best_cb_params)\n",
    "cb_tuned.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_train, y_train), (X_val, y_val)]    # both sets\n",
    ")\n",
    "\n",
    "cb_metrics, cb_train_pred, cb_val_pred, cb_test_pred = evaluate_model(\n",
    "    cb_tuned, X_train, y_train, X_val, y_val, X_test, y_test, \"CatBoost (Tuned)\"\n",
    ")\n",
    "\n",
    "models['CatBoost (Tuned)'] = cb_tuned"
   ],
   "id": "fdc7dab2f427e2a5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2026-02-16 14:00:59,101]\u001B[0m A new study created in memory with name: no-name-af8f50c0-054d-4250-a906-3190424088f3\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "OPTUNA TUNING CATBOOST (Validation-based, Strong Regularization)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "99bc5e08470947c4b4afce64216548a4"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2026-02-16 14:00:59,987]\u001B[0m Trial 0 finished with value: 60.8989262131448 and parameters: {'depth': 4, 'learning_rate': 0.009501137844669822, 'l2_leaf_reg': 14.295877763452735, 'bagging_temperature': 2.042360459953155, 'border_count': 53}. Best is trial 0 with value: 60.8989262131448.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:01:00,730]\u001B[0m Trial 1 finished with value: 74.79931477105445 and parameters: {'depth': 5, 'learning_rate': 0.0021618426276401393, 'l2_leaf_reg': 15.119717396548218, 'bagging_temperature': 1.6242519718110648, 'border_count': 40}. Best is trial 0 with value: 60.8989262131448.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:01:01,407]\u001B[0m Trial 2 finished with value: 65.63080725948832 and parameters: {'depth': 4, 'learning_rate': 0.004796435993054926, 'l2_leaf_reg': 6.543598196930246, 'bagging_temperature': 1.9552392013134077, 'border_count': 53}. Best is trial 0 with value: 60.8989262131448.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:01:02,149]\u001B[0m Trial 3 finished with value: 57.81697428380993 and parameters: {'depth': 4, 'learning_rate': 0.016951935504945313, 'l2_leaf_reg': 19.61498096961924, 'bagging_temperature': 2.6401386606826596, 'border_count': 32}. Best is trial 3 with value: 57.81697428380993.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:01:02,769]\u001B[0m Trial 4 finished with value: 64.70266223760024 and parameters: {'depth': 3, 'learning_rate': 0.007812200214475391, 'l2_leaf_reg': 13.74434050944246, 'bagging_temperature': 1.9875994163605015, 'border_count': 51}. Best is trial 3 with value: 57.81697428380993.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:01:03,343]\u001B[0m Trial 5 finished with value: 59.9343216071014 and parameters: {'depth': 3, 'learning_rate': 0.01721702143332914, 'l2_leaf_reg': 8.78805919185398, 'bagging_temperature': 2.85191524406104, 'border_count': 35}. Best is trial 3 with value: 57.81697428380993.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:01:04,016]\u001B[0m Trial 6 finished with value: 59.87590257338738 and parameters: {'depth': 4, 'learning_rate': 0.010555558884207051, 'l2_leaf_reg': 6.349038014334856, 'bagging_temperature': 2.1433238847876646, 'border_count': 43}. Best is trial 3 with value: 57.81697428380993.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:01:04,685]\u001B[0m Trial 7 finished with value: 78.30112896368615 and parameters: {'depth': 4, 'learning_rate': 0.0015672800837967511, 'l2_leaf_reg': 5.6231867231640305, 'bagging_temperature': 1.355464657256064, 'border_count': 35}. Best is trial 3 with value: 57.81697428380993.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:01:05,459]\u001B[0m Trial 8 finished with value: 64.23964483499878 and parameters: {'depth': 5, 'learning_rate': 0.005690513608051948, 'l2_leaf_reg': 18.516756521635752, 'bagging_temperature': 1.1094281466978866, 'border_count': 41}. Best is trial 3 with value: 57.81697428380993.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:01:06,054]\u001B[0m Trial 9 finished with value: 67.62512284305113 and parameters: {'depth': 3, 'learning_rate': 0.00510399643433897, 'l2_leaf_reg': 5.672385971902543, 'bagging_temperature': 2.0603653313567296, 'border_count': 64}. Best is trial 3 with value: 57.81697428380993.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:01:06,988]\u001B[0m Trial 10 finished with value: 54.126879592844936 and parameters: {'depth': 5, 'learning_rate': 0.029643409352144764, 'l2_leaf_reg': 8.863252627072754, 'bagging_temperature': 2.955198157844228, 'border_count': 63}. Best is trial 10 with value: 54.126879592844936.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:01:07,547]\u001B[0m Trial 11 finished with value: 56.26330045282624 and parameters: {'depth': 5, 'learning_rate': 0.027668921066304582, 'l2_leaf_reg': 9.3543324503596, 'bagging_temperature': 2.978210873850635, 'border_count': 64}. Best is trial 10 with value: 54.126879592844936.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:01:08,365]\u001B[0m Trial 12 finished with value: 54.04353626075624 and parameters: {'depth': 5, 'learning_rate': 0.029769365296039966, 'l2_leaf_reg': 9.13969716759149, 'bagging_temperature': 2.997681193777965, 'border_count': 63}. Best is trial 12 with value: 54.04353626075624.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:01:09,101]\u001B[0m Trial 13 finished with value: 54.65883375501312 and parameters: {'depth': 5, 'learning_rate': 0.02932762522475176, 'l2_leaf_reg': 7.88570072141165, 'bagging_temperature': 2.535252652239788, 'border_count': 58}. Best is trial 12 with value: 54.04353626075624.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:01:09,900]\u001B[0m Trial 14 finished with value: 56.03419883973219 and parameters: {'depth': 5, 'learning_rate': 0.018458383560398302, 'l2_leaf_reg': 11.575140290165693, 'bagging_temperature': 2.4682728420548834, 'border_count': 59}. Best is trial 12 with value: 54.04353626075624.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:01:10,719]\u001B[0m Trial 15 finished with value: 71.26759942748532 and parameters: {'depth': 5, 'learning_rate': 0.0027477848471447164, 'l2_leaf_reg': 11.126763867695853, 'bagging_temperature': 2.74725933910599, 'border_count': 59}. Best is trial 12 with value: 54.04353626075624.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:01:11,489]\u001B[0m Trial 16 finished with value: 55.270190045542755 and parameters: {'depth': 5, 'learning_rate': 0.020457632707993643, 'l2_leaf_reg': 8.042040380193056, 'bagging_temperature': 2.3736896870266952, 'border_count': 47}. Best is trial 12 with value: 54.04353626075624.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:01:12,285]\u001B[0m Trial 17 finished with value: 57.90827866619263 and parameters: {'depth': 5, 'learning_rate': 0.012201881601833732, 'l2_leaf_reg': 10.610014129444952, 'bagging_temperature': 2.92460382221747, 'border_count': 56}. Best is trial 12 with value: 54.04353626075624.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:01:13,059]\u001B[0m Trial 18 finished with value: 82.5285378371894 and parameters: {'depth': 4, 'learning_rate': 0.0011006012054806046, 'l2_leaf_reg': 6.7419665775253, 'bagging_temperature': 2.295090556842933, 'border_count': 62}. Best is trial 12 with value: 54.04353626075624.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:01:13,884]\u001B[0m Trial 19 finished with value: 54.38250342446518 and parameters: {'depth': 5, 'learning_rate': 0.024003992995465583, 'l2_leaf_reg': 7.762532897582056, 'bagging_temperature': 2.7080162467160203, 'border_count': 61}. Best is trial 12 with value: 54.04353626075624.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:01:14,656]\u001B[0m Trial 20 finished with value: 57.105530785052515 and parameters: {'depth': 5, 'learning_rate': 0.01457529427189419, 'l2_leaf_reg': 12.172918017975155, 'bagging_temperature': 1.7890264315933264, 'border_count': 48}. Best is trial 12 with value: 54.04353626075624.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:01:15,459]\u001B[0m Trial 21 finished with value: 54.68933941961316 and parameters: {'depth': 5, 'learning_rate': 0.024475801928506485, 'l2_leaf_reg': 7.489530392426001, 'bagging_temperature': 2.7841074254422846, 'border_count': 61}. Best is trial 12 with value: 54.04353626075624.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:01:16,322]\u001B[0m Trial 22 finished with value: 54.82832779393682 and parameters: {'depth': 5, 'learning_rate': 0.023726522024297984, 'l2_leaf_reg': 9.388826473030804, 'bagging_temperature': 2.9975453074881413, 'border_count': 56}. Best is trial 12 with value: 54.04353626075624.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:01:17,859]\u001B[0m Trial 23 finished with value: 54.17967441847104 and parameters: {'depth': 5, 'learning_rate': 0.029620459162627618, 'l2_leaf_reg': 7.347856566234595, 'bagging_temperature': 2.6365410180519135, 'border_count': 62}. Best is trial 12 with value: 54.04353626075624.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:01:18,722]\u001B[0m Trial 24 finished with value: 58.48685978797209 and parameters: {'depth': 4, 'learning_rate': 0.013646088654056079, 'l2_leaf_reg': 5.156662978565844, 'bagging_temperature': 2.580727395829765, 'border_count': 64}. Best is trial 12 with value: 54.04353626075624.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:01:19,498]\u001B[0m Trial 25 finished with value: 54.13415033795138 and parameters: {'depth': 5, 'learning_rate': 0.028664512693063596, 'l2_leaf_reg': 8.70811700372604, 'bagging_temperature': 2.8350457233032853, 'border_count': 56}. Best is trial 12 with value: 54.04353626075624.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:01:20,279]\u001B[0m Trial 26 finished with value: 60.67435223367329 and parameters: {'depth': 5, 'learning_rate': 0.00739117969183604, 'l2_leaf_reg': 8.794403569701768, 'bagging_temperature': 2.8264113527713945, 'border_count': 56}. Best is trial 12 with value: 54.04353626075624.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:01:20,949]\u001B[0m Trial 27 finished with value: 69.04926872500991 and parameters: {'depth': 4, 'learning_rate': 0.0035504069236393034, 'l2_leaf_reg': 9.909345893014285, 'bagging_temperature': 2.4160979580420805, 'border_count': 53}. Best is trial 12 with value: 54.04353626075624.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:01:21,725]\u001B[0m Trial 28 finished with value: 55.509126556852785 and parameters: {'depth': 5, 'learning_rate': 0.02051139135871697, 'l2_leaf_reg': 8.688865364194651, 'bagging_temperature': 2.2272657170836774, 'border_count': 58}. Best is trial 12 with value: 54.04353626075624.\u001B[0m\n",
      "\u001B[32m[I 2026-02-16 14:01:22,421]\u001B[0m Trial 29 finished with value: 60.377343875053924 and parameters: {'depth': 4, 'learning_rate': 0.00989206490425323, 'l2_leaf_reg': 12.831336968296194, 'bagging_temperature': 2.996837582945985, 'border_count': 51}. Best is trial 12 with value: 54.04353626075624.\u001B[0m\n",
      "\n",
      "Best CatBoost parameters: {'depth': 5, 'learning_rate': 0.029769365296039966, 'l2_leaf_reg': 9.13969716759149, 'bagging_temperature': 2.997681193777965, 'border_count': 63}\n",
      "Best validation RMSE: 54.0435\n",
      "\n",
      "==================================================\n",
      "CatBoost (Tuned) Performance:\n",
      "==================================================\n",
      "  Train RMSE: 49.2772 | Validation RMSE: 54.0435 | Test RMSE: 54.3103\n",
      "  Train MAE : 31.5821 | Validation MAE : 33.8848 | Test MAE : 34.3698\n",
      "  Train R2  : 0.7900 | Validation R2  : 0.7052 | Test R2  : 0.6827\n",
      "  Overfit Gap (Val-Train): 4.7664\n",
      "  Test Gap (Test-Train): 5.0331\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T08:31:23.339655900Z",
     "start_time": "2026-02-16T08:31:23.318243200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Combine all metrics into a DataFrame\n",
    "metrics_list = [xgb_metrics, lgb_metrics, cb_metrics]\n",
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "metrics_df.set_index('Model', inplace=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TUNED MODELS COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nTest Set Performance:\")\n",
    "print(metrics_df[['Test RMSE', 'Test MAE', 'Test R2']].round(4))\n",
    "\n",
    "# Identify best model based on test RMSE\n",
    "best_model = metrics_df['Test RMSE'].idxmin()\n",
    "best_rmse = metrics_df.loc[best_model, 'Test RMSE']\n",
    "print(f\"\\n Best Model by Test RMSE: {best_model} (RMSE: {best_rmse:.4f})\")"
   ],
   "id": "2258a58c587a9406",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TUNED MODELS COMPARISON\n",
      "============================================================\n",
      "\n",
      "Test Set Performance:\n",
      "                  Test RMSE  Test MAE  Test R2\n",
      "Model                                         \n",
      "XGBoost (Tuned)     52.2491   35.5702   0.7064\n",
      "LightGBM (Tuned)    49.6772   33.7795   0.7345\n",
      "CatBoost (Tuned)    54.3103   34.3698   0.6827\n",
      "\n",
      " Best Model by Test RMSE: LightGBM (Tuned) (RMSE: 49.6772)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T08:31:23.496878200Z",
     "start_time": "2026-02-16T08:31:23.341678500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extract evaluation history\n",
    "\n",
    "# XGBoost\n",
    "xgb_results = xgb_tuned.evals_result()\n",
    "xgb_train_rmse = xgb_results['validation_0']['rmse']\n",
    "xgb_val_rmse = xgb_results['validation_1']['rmse']\n",
    "\n",
    "# LightGBM\n",
    "lgb_results = lgb_tuned.evals_result_\n",
    "print(\"LightGBM evals_result_ keys:\", lgb_results.keys())\n",
    "lgb_train_key = 'training'\n",
    "lgb_val_key = [k for k in lgb_results.keys() if k != lgb_train_key][0]\n",
    "print(f\"Using validation key: {lgb_val_key}\")\n",
    "lgb_train_rmse = lgb_results[lgb_train_key]['rmse']\n",
    "lgb_val_rmse = lgb_results[lgb_val_key]['rmse']\n",
    "\n",
    "# CatBoost\n",
    "cb_results = cb_tuned.get_evals_result()\n",
    "print(\"CatBoost evals_result keys:\", cb_results.keys())\n",
    "cb_train_rmse = cb_results['learn']['RMSE']\n",
    "cb_val_rmse = cb_results['validation']['RMSE']\n",
    "\n",
    "# Plotting function\n",
    "def plot_learning_curve(ax, train_errors, val_errors, model_name):\n",
    "    epochs = range(1, len(train_errors)+1)\n",
    "    ax.plot(epochs, train_errors, label='Train RMSE', color='blue')\n",
    "    ax.plot(epochs, val_errors, label='Validation RMSE', color='orange')\n",
    "\n",
    "    best_epoch = np.argmin(val_errors) + 1\n",
    "    ax.axvline(x=best_epoch, color='green', linestyle='--', alpha=0.7, label=f'Best @ {best_epoch}')\n",
    "\n",
    "    if best_epoch < len(val_errors):\n",
    "        ax.axvspan(best_epoch, len(val_errors), alpha=0.2, color='red', label='Overfitting region')\n",
    "\n",
    "    final_gap = val_errors[-1] - train_errors[-1]\n",
    "    ax.text(0.05, 0.95, f'Final Gap: {final_gap:.2f}', transform=ax.transAxes,\n",
    "            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "    ax.set_title(f'{model_name} Learning Curve')\n",
    "    ax.set_xlabel('Boosting Rounds')\n",
    "    ax.set_ylabel('RMSE')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "plot_learning_curve(axes[0], xgb_train_rmse, xgb_val_rmse, 'XGBoost (Tuned)')\n",
    "plot_learning_curve(axes[1], lgb_train_rmse, lgb_val_rmse, 'LightGBM (Tuned)')\n",
    "plot_learning_curve(axes[2], cb_train_rmse, cb_val_rmse, 'CatBoost (Tuned)')\n",
    "plt.suptitle('Learning Curves with Overfitting Detection', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "142d21105d778e08",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM evals_result_ keys: dict_keys(['training', 'valid_1'])\n",
      "Using validation key: valid_1\n",
      "CatBoost evals_result keys: dict_keys(['learn', 'validation_0', 'validation_1'])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'validation'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 21\u001B[39m\n\u001B[32m     19\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mCatBoost evals_result keys:\u001B[39m\u001B[33m\"\u001B[39m, cb_results.keys())\n\u001B[32m     20\u001B[39m cb_train_rmse = cb_results[\u001B[33m'\u001B[39m\u001B[33mlearn\u001B[39m\u001B[33m'\u001B[39m][\u001B[33m'\u001B[39m\u001B[33mRMSE\u001B[39m\u001B[33m'\u001B[39m]\n\u001B[32m---> \u001B[39m\u001B[32m21\u001B[39m cb_val_rmse = \u001B[43mcb_results\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mvalidation\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m[\u001B[33m'\u001B[39m\u001B[33mRMSE\u001B[39m\u001B[33m'\u001B[39m]\n\u001B[32m     23\u001B[39m \u001B[38;5;66;03m# Plotting function\u001B[39;00m\n\u001B[32m     24\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mplot_learning_curve\u001B[39m(ax, train_errors, val_errors, model_name):\n",
      "\u001B[31mKeyError\u001B[39m: 'validation'"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "# XGBoost\n",
    "axes[0].plot(xgb_train_rmse, label='Train Loss', color='blue')\n",
    "axes[0].plot(xgb_val_rmse, label='Validation Loss', color='red')\n",
    "axes[0].fill_between(range(len(xgb_train_rmse)), xgb_train_rmse, xgb_val_rmse, alpha=0.3, color='gray')\n",
    "axes[0].set_title('XGBoost Loss Curve')\n",
    "axes[0].set_xlabel('Iterations')\n",
    "axes[0].set_ylabel('RMSE')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# LightGBM\n",
    "axes[1].plot(lgb_train_rmse, label='Train Loss', color='blue')\n",
    "axes[1].plot(lgb_val_rmse, label='Validation Loss', color='red')\n",
    "axes[1].fill_between(range(len(lgb_train_rmse)), lgb_train_rmse, lgb_val_rmse, alpha=0.3, color='gray')\n",
    "axes[1].set_title('LightGBM Loss Curve')\n",
    "axes[1].set_xlabel('Iterations')\n",
    "axes[1].set_ylabel('RMSE')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# CatBoost\n",
    "axes[2].plot(cb_train_rmse, label='Train Loss', color='blue')\n",
    "axes[2].plot(cb_val_rmse, label='Validation Loss', color='red')\n",
    "axes[2].fill_between(range(len(cb_train_rmse)), cb_train_rmse, cb_val_rmse, alpha=0.3, color='gray')\n",
    "axes[2].set_title('CatBoost Loss Curve')\n",
    "axes[2].set_xlabel('Iterations')\n",
    "axes[2].set_ylabel('RMSE')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Training vs Validation Loss Curves', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "2db40f30456c3cfc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "residuals = {\n",
    "    'XGBoost (Tuned)': y_test - xgb_test_pred,\n",
    "    'LightGBM (Tuned)': y_test - lgb_test_pred,\n",
    "    'CatBoost (Tuned)': y_test - cb_test_pred\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "for i, (name, resid) in enumerate(residuals.items()):\n",
    "    # Scatter\n",
    "    ax = axes[0, i]\n",
    "    ax.scatter(y_test, resid, alpha=0.5, s=20)\n",
    "    ax.axhline(y=0, color='red', linestyle='--')\n",
    "    ax.set_title(f'{name} Residuals')\n",
    "    ax.set_xlabel('Actual Price')\n",
    "    ax.set_ylabel('Residuals')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Histogram\n",
    "    ax = axes[1, i]\n",
    "    ax.hist(resid, bins=30, edgecolor='black', alpha=0.7)\n",
    "    ax.axvline(x=0, color='red', linestyle='--')\n",
    "    ax.set_title(f'{name} Residual Distribution')\n",
    "    ax.set_xlabel('Residuals')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    mean_resid = np.mean(resid)\n",
    "    std_resid = np.std(resid)\n",
    "    ax.text(0.05, 0.95, f'Mean: {mean_resid:.2f}\\nStd: {std_resid:.2f}',\n",
    "            transform=ax.transAxes, fontsize=9, verticalalignment='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "plt.suptitle('Residual Analysis', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "e1600e423d55acd5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "predictions = {\n",
    "    'XGBoost (Tuned)': xgb_test_pred,\n",
    "    'LightGBM (Tuned)': lgb_test_pred,\n",
    "    'CatBoost (Tuned)': cb_test_pred\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for ax, (name, pred) in zip(axes, predictions.items()):\n",
    "    ax.scatter(y_test, pred, alpha=0.5, s=20)\n",
    "    min_val = min(y_test.min(), pred.min())\n",
    "    max_val = max(y_test.max(), pred.max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', label='Perfect')\n",
    "    r2 = r2_score(y_test, pred)\n",
    "    ax.set_title(f'{name}\\nActual vs Predicted (R = {r2:.4f})')\n",
    "    ax.set_xlabel('Actual Price')\n",
    "    ax.set_ylabel('Predicted Price')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Test Set: Actual vs Predicted', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "59b9af527a25dcf7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from math import pi\n",
    "\n",
    "# Prepare metrics for radar (normalize)\n",
    "metrics_for_radar = ['Test RMSE', 'Test MAE', 'Test R2']\n",
    "xgb_vals = [xgb_metrics['Test RMSE'], xgb_metrics['Test MAE'], xgb_metrics['Test R2']]\n",
    "lgb_vals = [lgb_metrics['Test RMSE'], lgb_metrics['Test MAE'], lgb_metrics['Test R2']]\n",
    "cb_vals = [cb_metrics['Test RMSE'], cb_metrics['Test MAE'], cb_metrics['Test R2']]\n",
    "\n",
    "# Normalize (1 is best for all after transformation)\n",
    "max_rmse = max(xgb_vals[0], lgb_vals[0], cb_vals[0])\n",
    "max_mae = max(xgb_vals[1], lgb_vals[1], cb_vals[1])\n",
    "xgb_norm = [1 - xgb_vals[0]/max_rmse, 1 - xgb_vals[1]/max_mae, xgb_vals[2]]\n",
    "lgb_norm = [1 - lgb_vals[0]/max_rmse, 1 - lgb_vals[1]/max_mae, lgb_vals[2]]\n",
    "cb_norm = [1 - cb_vals[0]/max_rmse, 1 - cb_vals[1]/max_mae, cb_vals[2]]\n",
    "\n",
    "N = len(metrics_for_radar)\n",
    "angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "angles += angles[:1]\n",
    "\n",
    "xgb_norm += xgb_norm[:1]\n",
    "lgb_norm += lgb_norm[:1]\n",
    "cb_norm += cb_norm[:1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "ax.plot(angles, xgb_norm, 'o-', linewidth=2, label='XGBoost', color='blue')\n",
    "ax.fill(angles, xgb_norm, alpha=0.1, color='blue')\n",
    "ax.plot(angles, lgb_norm, 'o-', linewidth=2, label='LightGBM', color='green')\n",
    "ax.fill(angles, lgb_norm, alpha=0.1, color='green')\n",
    "ax.plot(angles, cb_norm, 'o-', linewidth=2, label='CatBoost', color='orange')\n",
    "ax.fill(angles, cb_norm, alpha=0.1, color='orange')\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(metrics_for_radar, fontsize=12)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('Model Comparison Radar Chart\\n(Higher is better)', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "9205e149b9436e57",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Assess overfitting and accuracy\n",
    "def assess_overfitting(train_rmse, val_rmse, test_rmse):\n",
    "    val_gap = val_rmse - train_rmse\n",
    "    test_gap = test_rmse - train_rmse\n",
    "    val_gap_pct = (val_gap / train_rmse) * 100\n",
    "    test_gap_pct = (test_gap / train_rmse) * 100\n",
    "    avg_gap_pct = (val_gap_pct + test_gap_pct) / 2\n",
    "    if avg_gap_pct < 5:\n",
    "        return \"LOW\", \" Generalizes very well\"\n",
    "    elif avg_gap_pct < 15:\n",
    "        return \"MODERATE\", \" Some overfitting, acceptable\"\n",
    "    else:\n",
    "        return \"HIGH\", \" Significant overfitting\"\n",
    "\n",
    "def accuracy_rating(r2):\n",
    "    if r2 >= 0.9:\n",
    "        return \"EXCELLENT\", \" Very high accuracy\"\n",
    "    elif r2 >= 0.8:\n",
    "        return \"GOOD\", \" Good predictive power\"\n",
    "    elif r2 >= 0.6:\n",
    "        return \"FAIR\", \" Moderate accuracy\"\n",
    "    elif r2 >= 0.4:\n",
    "        return \"POOR\", \" Low accuracy\"\n",
    "    else:\n",
    "        return \"VERY POOR\", \" Needs improvement\"\n",
    "\n",
    "# Collect details\n",
    "model_details = []\n",
    "for model_name, model in models.items():\n",
    "    if 'XGBoost' in model_name:\n",
    "        train_rmse = xgb_metrics['Train RMSE']\n",
    "        val_rmse = xgb_metrics['Validation RMSE']\n",
    "        test_rmse = xgb_metrics['Test RMSE']\n",
    "        test_r2 = xgb_metrics['Test R2']\n",
    "        test_mae = xgb_metrics['Test MAE']\n",
    "    elif 'LightGBM' in model_name:\n",
    "        train_rmse = lgb_metrics['Train RMSE']\n",
    "        val_rmse = lgb_metrics['Validation RMSE']\n",
    "        test_rmse = lgb_metrics['Test RMSE']\n",
    "        test_r2 = lgb_metrics['Test R2']\n",
    "        test_mae = lgb_metrics['Test MAE']\n",
    "    else:\n",
    "        train_rmse = cb_metrics['Train RMSE']\n",
    "        val_rmse = cb_metrics['Validation RMSE']\n",
    "        test_rmse = cb_metrics['Test RMSE']\n",
    "        test_r2 = cb_metrics['Test R2']\n",
    "        test_mae = cb_metrics['Test MAE']\n",
    "\n",
    "    overfit_level, overfit_desc = assess_overfitting(train_rmse, val_rmse, test_rmse)\n",
    "    acc_rating, acc_desc = accuracy_rating(test_r2)\n",
    "\n",
    "    model_details.append({\n",
    "        'Model': model_name,\n",
    "        'Train RMSE': train_rmse,\n",
    "        'Val RMSE': val_rmse,\n",
    "        'Test RMSE': test_rmse,\n",
    "        'Test MAE': test_mae,\n",
    "        'Test R': test_r2,\n",
    "        'Overfitting Level': overfit_level,\n",
    "        'Overfitting Description': overfit_desc,\n",
    "        'Accuracy Rating': acc_rating,\n",
    "        'Accuracy Description': acc_desc\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(model_details)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\" FINAL TUNED MODELS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Composite score for best model selection\n",
    "summary_df['RMSE_norm'] = 1 - (summary_df['Test RMSE'] / summary_df['Test RMSE'].max())\n",
    "summary_df['R2_norm'] = summary_df['Test R'] / summary_df['Test R'].max()\n",
    "overfit_map = {'LOW': 1.0, 'MODERATE': 0.6, 'HIGH': 0.2}\n",
    "summary_df['Overfit_score'] = summary_df['Overfitting Level'].map(overfit_map)\n",
    "summary_df['Composite'] = 0.4*summary_df['R2_norm'] + 0.3*summary_df['RMSE_norm'] + 0.3*summary_df['Overfit_score']\n",
    "\n",
    "best_idx = summary_df['Composite'].idxmax()\n",
    "best_model = summary_df.loc[best_idx]\n",
    "\n",
    "print(f\"\\n BEST MODEL (by composite score): {best_model['Model']}\")\n",
    "print(f\"   Test RMSE: {best_model['Test RMSE']:.4f}, Test R: {best_model['Test R']:.4f}\")\n",
    "print(f\"   Overfitting: {best_model['Overfitting Level']}, Accuracy: {best_model['Accuracy Rating']}\")\n",
    "\n",
    "# Save tuned models\n",
    "models_dir = \"../models/tuned_models\"\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    safe_name = model_name.replace(' ', '_').replace('(', '').replace(')', '').lower()\n",
    "    filename = f\"{safe_name}.joblib\"\n",
    "    joblib.dump(model, os.path.join(models_dir, filename))\n",
    "    print(f\" Saved: {filename}\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = summary_df[['Model', 'Test RMSE', 'Test MAE', 'Test R', 'Overfitting Level', 'Accuracy Rating']].copy()\n",
    "metadata['Best_Model'] = best_model['Model']\n",
    "metadata['Generated'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "metadata.to_csv(os.path.join(models_dir, 'model_metadata.csv'), index=False)\n",
    "print(f\" Saved model metadata\")\n",
    "\n",
    "print(f\"\\n All tuned models saved to: {os.path.abspath(models_dir)}\")"
   ],
   "id": "d995c1271842c27",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
