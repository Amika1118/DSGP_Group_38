{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T17:36:30.516166800Z",
     "start_time": "2026-01-31T17:36:30.419308300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create processed directory\n",
    "processed_dir = \"../data/processed/\"\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "# Load data\n",
    "data_path = \"../data/raw data/dambulla_daily_vegetable_prices_2010_2025 (1).csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Date range: {df['Date'].min()} to {df['Date'].max()}\")\n",
    "print(f\"Unique vegetables: {df['Vegetable_Name'].nunique()}\")"
   ],
   "id": "2756077044b88284",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (35166, 7)\n",
      "Date range: 2010-01-01 00:00:00 to 2026-01-17 00:00:00\n",
      "Unique vegetables: 6\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T17:36:30.563196400Z",
     "start_time": "2026-01-31T17:36:30.519168100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_weekly_features_corrected(df):\n",
    "    \"\"\"\n",
    "    Convert daily data to weekly with correct ISO week handling\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # CORRECTED: Use ISO year and week consistently\n",
    "    iso_calendar = df['Date'].dt.isocalendar()\n",
    "    df['ISO_Year'] = iso_calendar['year']\n",
    "    df['ISO_Week'] = iso_calendar['week']\n",
    "\n",
    "    # Create consistent week identifier\n",
    "    df['Week_ID'] = df['ISO_Year'].astype(str) + '-W' + df['ISO_Week'].astype(str).str.zfill(2)\n",
    "\n",
    "    # Use Monday as start of week (consistent with ISO)\n",
    "    df['Week_Start'] = df['Date'] - pd.to_timedelta(df['Date'].dt.dayofweek, unit='D')\n",
    "\n",
    "    return df\n",
    "\n",
    "df = create_weekly_features_corrected(df)\n",
    "print(f\"Corrected week features created\")\n",
    "print(f\"Sample Week_ID: {df['Week_ID'].iloc[0]}\")\n",
    "print(f\"Week_Start sample: {df['Week_Start'].iloc[0]}\")"
   ],
   "id": "5565095a16abe7fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected week features created\n",
      "Sample Week_ID: 2009-W53\n",
      "Week_Start sample: 2009-12-28 00:00:00\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T17:36:36.667568800Z",
     "start_time": "2026-01-31T17:36:30.575194Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def aggregate_weekly_safe(df, test_split_date='2023-01-01'):\n",
    "    \"\"\"\n",
    "    Aggregate with leak-proof statistics\n",
    "    \"\"\"\n",
    "    weekly_features = []\n",
    "    test_split_date = pd.Timestamp(test_split_date)\n",
    "\n",
    "    for veg_name in df['Vegetable_Name'].unique():\n",
    "        veg_df = df[df['Vegetable_Name'] == veg_name].copy()\n",
    "\n",
    "        # Separate train/test before aggregation\n",
    "        veg_train = veg_df[veg_df['Date'] < test_split_date]\n",
    "        veg_test = veg_df[veg_df['Date'] >= test_split_date]\n",
    "\n",
    "        # Function to aggregate weekly\n",
    "        def aggregate_group(group_df):\n",
    "            return pd.Series({\n",
    "                'Weekly_Arrival_Sum': group_df['Daily_Arrival_MT'].sum(),\n",
    "                'Avg_Daily_Arrival': group_df['Daily_Arrival_MT'].mean(),\n",
    "                'Std_Daily_Arrival': group_df['Daily_Arrival_MT'].std(),\n",
    "                'Min_Daily_Arrival': group_df['Daily_Arrival_MT'].min(),\n",
    "                'Max_Daily_Arrival': group_df['Daily_Arrival_MT'].max(),\n",
    "                'Weekly_Sales_Sum': group_df['Estimated_Sales_MT'].sum(),\n",
    "                'Avg_Daily_Sales': group_df['Estimated_Sales_MT'].mean(),\n",
    "                'Std_Daily_Sales': group_df['Estimated_Sales_MT'].std(),\n",
    "                'Min_Daily_Sales': group_df['Estimated_Sales_MT'].min(),\n",
    "                'Max_Daily_Sales': group_df['Estimated_Sales_MT'].max(),\n",
    "                'Avg_Weekly_Price': group_df['Wholesale_Price_Rs_kg'].mean(),\n",
    "                'Std_Weekly_Price': group_df['Wholesale_Price_Rs_kg'].std(),\n",
    "                'Min_Weekly_Price': group_df['Wholesale_Price_Rs_kg'].min(),\n",
    "                'Max_Weekly_Price': group_df['Wholesale_Price_Rs_kg'].max(),\n",
    "                'Season': group_df['Season'].iloc[0],\n",
    "                'Weekly_Supply_Status': group_df['Supply_Status'].mode()[0] if not group_df['Supply_Status'].mode().empty else 'Stable'\n",
    "            })\n",
    "\n",
    "        # Aggregate train and test separately\n",
    "        train_weekly = veg_train.groupby(['Week_ID', 'Week_Start', 'ISO_Year', 'ISO_Week']).apply(aggregate_group).reset_index()\n",
    "        test_weekly = veg_test.groupby(['Week_ID', 'Week_Start', 'ISO_Year', 'ISO_Week']).apply(aggregate_group).reset_index()\n",
    "\n",
    "        # Combine and add vegetable name\n",
    "        veg_weekly = pd.concat([train_weekly, test_weekly])\n",
    "        veg_weekly['Vegetable_Name'] = veg_name\n",
    "        veg_weekly['Is_Train'] = veg_weekly['Week_Start'] < test_split_date\n",
    "\n",
    "        weekly_features.append(veg_weekly)\n",
    "\n",
    "    # Combine all vegetables\n",
    "    weekly_df = pd.concat(weekly_features, ignore_index=True)\n",
    "    weekly_df = weekly_df.sort_values(['Vegetable_Name', 'Week_Start'])\n",
    "\n",
    "    return weekly_df\n",
    "\n",
    "weekly_df = aggregate_weekly_safe(df)\n",
    "print(f\"\\nWeekly dataset shape: {weekly_df.shape}\")\n",
    "print(f\"Train samples: {weekly_df['Is_Train'].sum()}\")\n",
    "print(f\"Test samples: {(~weekly_df['Is_Train']).sum()}\")"
   ],
   "id": "29b634ccca0f8e30",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weekly dataset shape: (5034, 22)\n",
      "Train samples: 4080\n",
      "Test samples: 954\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T17:36:43.749609100Z",
     "start_time": "2026-01-31T17:36:36.759620200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_lag_features_safe(weekly_df, lags=[1, 2, 4, 8, 12, 26, 52]):\n",
    "    \"\"\"\n",
    "    Create lag features without train-test leakage\n",
    "    \"\"\"\n",
    "    df = weekly_df.copy()\n",
    "\n",
    "    # Sort to ensure proper lag calculation\n",
    "    df = df.sort_values(['Vegetable_Name', 'Week_Start'])\n",
    "\n",
    "    # Create lag features for each vegetable separately\n",
    "    for veg_name in df['Vegetable_Name'].unique():\n",
    "        veg_mask = df['Vegetable_Name'] == veg_name\n",
    "\n",
    "        # Separate train and test indices\n",
    "        train_mask = veg_mask & df['Is_Train']\n",
    "        test_mask = veg_mask & (~df['Is_Train'])\n",
    "\n",
    "        # Key variables to create lags for\n",
    "        target_vars = ['Weekly_Arrival_Sum', 'Weekly_Sales_Sum', 'Avg_Weekly_Price']\n",
    "\n",
    "        for var in target_vars:\n",
    "            for lag in lags:\n",
    "                col_name = f'{var}_lag_{lag}w'\n",
    "\n",
    "                # Create lag for entire series\n",
    "                lag_values = df.loc[veg_mask, var].shift(lag)\n",
    "                df.loc[veg_mask, col_name] = lag_values\n",
    "\n",
    "                # For test data, we can optionally forward-fill from last train value\n",
    "                # But better to leave as NaN initially\n",
    "                # We'll handle NaN filling separately\n",
    "\n",
    "    # Rolling statistics - compute within train/test separately\n",
    "    for veg_name in df['Vegetable_Name'].unique():\n",
    "        veg_mask = df['Vegetable_Name'] == veg_name\n",
    "\n",
    "        # Separate train and test\n",
    "        veg_df = df[veg_mask].copy()\n",
    "        train_idx = veg_df[veg_df['Is_Train']].index\n",
    "        test_idx = veg_df[~veg_df['Is_Train']].index\n",
    "\n",
    "        for window in [4, 8, 12, 26]:\n",
    "            # Compute rolling stats separately for train and test\n",
    "            # For train: use only train data\n",
    "            if len(train_idx) >= window:\n",
    "                train_arrival_ma = veg_df.loc[train_idx, 'Weekly_Arrival_Sum'].rolling(window=window, min_periods=1).mean()\n",
    "                df.loc[train_idx, f'Weekly_Arrival_Sum_MA_{window}w'] = train_arrival_ma\n",
    "\n",
    "                train_price_ma = veg_df.loc[train_idx, 'Avg_Weekly_Price'].rolling(window=window, min_periods=1).mean()\n",
    "                df.loc[train_idx, f'Avg_Weekly_Price_MA_{window}w'] = train_price_ma\n",
    "\n",
    "                train_arrival_std = veg_df.loc[train_idx, 'Weekly_Arrival_Sum'].rolling(window=window, min_periods=1).std()\n",
    "                df.loc[train_idx, f'Weekly_Arrival_Sum_Std_{window}w'] = train_arrival_std\n",
    "\n",
    "            # For test: use expanding window from last train values\n",
    "            if len(test_idx) > 0:\n",
    "                # Use all available data up to each point, but don't look into future\n",
    "                for idx in test_idx:\n",
    "                    current_date = df.loc[idx, 'Week_Start']\n",
    "                    historical_mask = (df['Vegetable_Name'] == veg_name) & (df['Week_Start'] < current_date)\n",
    "\n",
    "                    if historical_mask.sum() >= window:\n",
    "                        historical_arrival = df.loc[historical_mask, 'Weekly_Arrival_Sum'].tail(window)\n",
    "                        df.loc[idx, f'Weekly_Arrival_Sum_MA_{window}w'] = historical_arrival.mean()\n",
    "                        df.loc[idx, f'Avg_Weekly_Price_MA_{window}w'] = df.loc[historical_mask, 'Avg_Weekly_Price'].tail(window).mean()\n",
    "                        df.loc[idx, f'Weekly_Arrival_Sum_Std_{window}w'] = historical_arrival.std()\n",
    "\n",
    "    return df\n",
    "\n",
    "weekly_df = create_lag_features_safe(weekly_df)\n",
    "print(f\"\\nLag features created safely\")\n",
    "print(f\"Missing values in lag features: {weekly_df[[col for col in weekly_df.columns if 'lag' in col]].isnull().sum().sum()}\")"
   ],
   "id": "b2b7d1813cfd5377",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lag features created safely\n",
      "Missing values in lag features: 1890\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T17:36:49.158704100Z",
     "start_time": "2026-01-31T17:36:43.789110900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_statistical_features_safe(weekly_df):\n",
    "    \"\"\"\n",
    "    Create statistical features with clipping for stability\n",
    "    \"\"\"\n",
    "    df = weekly_df.copy()\n",
    "\n",
    "    # 1. Supply-Demand Ratio (safe)\n",
    "    df['Supply_Demand_Ratio'] = df['Weekly_Arrival_Sum'] / (df['Weekly_Sales_Sum'] + 1e-6)\n",
    "\n",
    "    # 2. Price-Arrival Elasticity with clipping\n",
    "    df['Price_Change_Pct'] = df.groupby('Vegetable_Name')['Avg_Weekly_Price'].pct_change() * 100\n",
    "    df['Arrival_Change_Pct'] = df.groupby('Vegetable_Name')['Weekly_Arrival_Sum'].pct_change() * 100\n",
    "\n",
    "    # CORRECTED: Handle division by zero safely\n",
    "    epsilon = 1e-6\n",
    "    df['Price_Arrival_Elasticity'] = df['Price_Change_Pct'] / (df['Arrival_Change_Pct'] + epsilon * np.sign(df['Arrival_Change_Pct']))\n",
    "\n",
    "    # Clip extreme values\n",
    "    df['Price_Arrival_Elasticity'] = np.clip(df['Price_Arrival_Elasticity'], -10, 10)\n",
    "\n",
    "    # Alternative: Use log elasticity for better distribution\n",
    "    df['Log_Price_Arrival_Elasticity'] = np.log1p(np.abs(df['Price_Arrival_Elasticity'])) * np.sign(df['Price_Arrival_Elasticity'])\n",
    "\n",
    "    # 3. Volatility measures\n",
    "    df['Price_Volatility'] = df['Std_Weekly_Price'] / (df['Avg_Weekly_Price'] + epsilon)\n",
    "    df['Arrival_Volatility'] = df['Std_Daily_Arrival'] / (df['Avg_Daily_Arrival'] + epsilon)\n",
    "\n",
    "    # 4. Range features\n",
    "    df['Price_Range'] = df['Max_Weekly_Price'] - df['Min_Weekly_Price']\n",
    "    df['Arrival_Range'] = df['Max_Daily_Arrival'] - df['Min_Daily_Arrival']\n",
    "\n",
    "    # 5. Utilization rate (clipped)\n",
    "    df['Utilization_Rate'] = df['Weekly_Sales_Sum'] / (df['Weekly_Arrival_Sum'] + epsilon)\n",
    "    df['Utilization_Rate'] = np.clip(df['Utilization_Rate'], 0, 2)  # Rarely exceeds 1, but safe\n",
    "\n",
    "    # 6. Price position within week's range\n",
    "    df['Price_Position'] = (df['Avg_Weekly_Price'] - df['Min_Weekly_Price']) / (df['Price_Range'] + epsilon)\n",
    "    df['Price_Position'] = np.clip(df['Price_Position'], 0, 1)\n",
    "\n",
    "    # 7. Train-safe trend features\n",
    "    for veg_name in df['Vegetable_Name'].unique():\n",
    "        veg_mask = df['Vegetable_Name'] == veg_name\n",
    "        train_mask = veg_mask & df['Is_Train']\n",
    "        test_mask = veg_mask & (~df['Is_Train'])\n",
    "\n",
    "        # For training data\n",
    "        train_idx = df[train_mask].index\n",
    "        if len(train_idx) >= 4:\n",
    "            for i, idx in enumerate(train_idx):\n",
    "                if i >= 3:\n",
    "                    window_idx = train_idx[i-3:i+1]\n",
    "                    x = np.arange(4)\n",
    "                    y_price = df.loc[window_idx, 'Avg_Weekly_Price'].values\n",
    "                    y_arrival = df.loc[window_idx, 'Weekly_Arrival_Sum'].values\n",
    "\n",
    "                    # Linear regression slope\n",
    "                    slope_price = np.polyfit(x, y_price, 1)[0]\n",
    "                    slope_arrival = np.polyfit(x, y_arrival, 1)[0]\n",
    "\n",
    "                    df.loc[idx, 'Price_Trend_4w'] = slope_price\n",
    "                    df.loc[idx, 'Arrival_Trend_4w'] = slope_arrival\n",
    "\n",
    "        # For test data - only use past information\n",
    "        test_idx = df[test_mask].index\n",
    "        for idx in test_idx:\n",
    "            current_date = df.loc[idx, 'Week_Start']\n",
    "            past_mask = (df['Vegetable_Name'] == veg_name) & (df['Week_Start'] < current_date)\n",
    "            past_idx = df[past_mask].index\n",
    "\n",
    "            if len(past_idx) >= 4:\n",
    "                window_idx = past_idx[-4:]\n",
    "                x = np.arange(4)\n",
    "                y_price = df.loc[window_idx, 'Avg_Weekly_Price'].values\n",
    "                y_arrival = df.loc[window_idx, 'Weekly_Arrival_Sum'].values\n",
    "\n",
    "                slope_price = np.polyfit(x, y_price, 1)[0]\n",
    "                slope_arrival = np.polyfit(x, y_arrival, 1)[0]\n",
    "\n",
    "                df.loc[idx, 'Price_Trend_4w'] = slope_price\n",
    "                df.loc[idx, 'Arrival_Trend_4w'] = slope_arrival\n",
    "\n",
    "    return df\n",
    "\n",
    "weekly_df = create_statistical_features_safe(weekly_df)\n",
    "print(f\"\\nStatistical features created safely\")\n",
    "print(f\"Price_Arrival_Elasticity range: [{weekly_df['Price_Arrival_Elasticity'].min():.2f}, {weekly_df['Price_Arrival_Elasticity'].max():.2f}]\")"
   ],
   "id": "18452a487aeb6fa7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistical features created safely\n",
      "Price_Arrival_Elasticity range: [-10.00, 10.00]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T17:36:49.275375Z",
     "start_time": "2026-01-31T17:36:49.182818500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_features_leak_proof(weekly_df):\n",
    "    \"\"\"\n",
    "    Create features without leaking test information\n",
    "    \"\"\"\n",
    "    df = weekly_df.copy()\n",
    "\n",
    "    # Ensure we have the Month column from Week_Start\n",
    "    if 'Month' not in df.columns:\n",
    "        df['Month'] = df['Week_Start'].dt.month\n",
    "\n",
    "    # 1. Train-only statistics for encoding\n",
    "    train_df = df[df['Is_Train']].copy()\n",
    "\n",
    "    # Vegetable-specific statistics from TRAIN ONLY\n",
    "    veg_stats_train = train_df.groupby('Vegetable_Name').agg({\n",
    "        'Weekly_Arrival_Sum': ['mean', 'std'],\n",
    "        'Avg_Weekly_Price': ['mean', 'std'],\n",
    "        'Weekly_Sales_Sum': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "    # Flatten column names\n",
    "    veg_stats_train.columns = ['Vegetable_Name',\n",
    "                                'Veg_Train_Avg_Arrival', 'Veg_Train_Std_Arrival',\n",
    "                                'Veg_Train_Avg_Price', 'Veg_Train_Std_Price',\n",
    "                                'Veg_Train_Avg_Sales']\n",
    "\n",
    "    # Merge stats back (from train only)\n",
    "    df = pd.merge(df, veg_stats_train, on='Vegetable_Name', how='left')\n",
    "\n",
    "    # 2. Z-scores using train statistics\n",
    "    epsilon = 1e-6\n",
    "    df['Arrival_Z_Score'] = (df['Weekly_Arrival_Sum'] - df['Veg_Train_Avg_Arrival']) / (df['Veg_Train_Std_Arrival'] + epsilon)\n",
    "    df['Price_Z_Score'] = (df['Avg_Weekly_Price'] - df['Veg_Train_Avg_Price']) / (df['Veg_Train_Std_Price'] + epsilon)\n",
    "\n",
    "    # 3. Month-year statistics from TRAIN ONLY\n",
    "    # Ensure Month column exists in train_df\n",
    "    if 'Month' not in train_df.columns:\n",
    "        train_df['Month'] = train_df['Week_Start'].dt.month\n",
    "\n",
    "    month_year_stats_train = train_df.groupby(['ISO_Year', 'Month']).agg({\n",
    "        'Weekly_Arrival_Sum': 'mean',\n",
    "        'Avg_Weekly_Price': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "    month_year_stats_train.columns = ['ISO_Year', 'Month', 'MonthYear_Train_Avg_Arrival', 'MonthYear_Train_Avg_Price']\n",
    "\n",
    "    # Merge with main dataframe\n",
    "    df = pd.merge(df, month_year_stats_train, on=['ISO_Year', 'Month'], how='left')\n",
    "\n",
    "    # 4. Deviation from train monthly averages\n",
    "    df['Arrival_Dev_From_TrainMonthAvg'] = df['Weekly_Arrival_Sum'] - df['MonthYear_Train_Avg_Arrival']\n",
    "    df['Price_Dev_From_TrainMonthAvg'] = df['Avg_Weekly_Price'] - df['MonthYear_Train_Avg_Price']\n",
    "\n",
    "    # 5. Simple categorical encoding (no leakage)\n",
    "    supply_status_map = {'Low': 0, 'Stable': 1, 'High': 2}\n",
    "    df['Supply_Status_Encoded'] = df['Weekly_Supply_Status'].map(supply_status_map)\n",
    "\n",
    "    season_map = {'Maha': 0, 'Yala': 1}\n",
    "    df['Season_Encoded'] = df['Season'].map(season_map)\n",
    "\n",
    "    # 6. Cyclical encoding (no leakage risk)\n",
    "    # Ensure Month column exists\n",
    "    if 'Month' not in df.columns:\n",
    "        df['Month'] = df['Week_Start'].dt.month\n",
    "\n",
    "    df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
    "    df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
    "\n",
    "    # Ensure WeekOfYear exists\n",
    "    if 'WeekOfYear' not in df.columns:\n",
    "        df['WeekOfYear'] = df['Week_Start'].dt.isocalendar().week\n",
    "\n",
    "    df['Week_sin'] = np.sin(2 * np.pi * df['WeekOfYear'] / 52.1429)  # Average weeks per year\n",
    "    df['Week_cos'] = np.cos(2 * np.pi * df['WeekOfYear'] / 52.1429)\n",
    "\n",
    "    # 7. Safe interaction terms\n",
    "    df['Arrival_Price_Interaction'] = df['Weekly_Arrival_Sum'] * df['Avg_Weekly_Price']\n",
    "    df['Season_Arrival_Interaction'] = df['Season_Encoded'] * df['Weekly_Arrival_Sum']\n",
    "\n",
    "    return df\n",
    "\n",
    "# Now run the function\n",
    "weekly_df = create_features_leak_proof(weekly_df)\n",
    "print(f\"\\nLeak-proof features created\")\n",
    "print(f\"Z-score range (should be centered around 0): [{weekly_df['Price_Z_Score'].min():.2f}, {weekly_df['Price_Z_Score'].max():.2f}]\")"
   ],
   "id": "902a40a3863b840",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Leak-proof features created\n",
      "Z-score range (should be centered around 0): [-0.97, 2.89]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T17:36:49.741907500Z",
     "start_time": "2026-01-31T17:36:49.278368800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def handle_missing_values_robust(weekly_df):\n",
    "    \"\"\"\n",
    "    Handle NaN values with train-test separation\n",
    "    \"\"\"\n",
    "    df = weekly_df.copy()\n",
    "\n",
    "    print(f\"\\nMissing values before handling:\")\n",
    "    missing_counts = df.isnull().sum()\n",
    "    missing_cols = missing_counts[missing_counts > 0]\n",
    "    print(f\"Columns with missing values: {len(missing_cols)}\")\n",
    "\n",
    "    # Separate train and test\n",
    "    train_mask = df['Is_Train']\n",
    "    test_mask = ~df['Is_Train']\n",
    "\n",
    "    # 1. Forward fill for lag features (train only propagates to test start)\n",
    "    lag_cols = [col for col in df.columns if 'lag' in col or 'MA_' in col or 'Trend' in col]\n",
    "\n",
    "    for col in lag_cols:\n",
    "        if col in df.columns:\n",
    "            # Forward fill within each vegetable\n",
    "            df[col] = df.groupby('Vegetable_Name')[col].transform(lambda x: x.ffill())\n",
    "\n",
    "    # 2. For each vegetable, fill train with median, test with last train value\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    numeric_cols = [col for col in numeric_cols if col not in ['ISO_Year', 'ISO_Week', 'Month', 'WeekOfYear']]\n",
    "\n",
    "    for veg_name in df['Vegetable_Name'].unique():\n",
    "        veg_mask = df['Vegetable_Name'] == veg_name\n",
    "        veg_train_mask = veg_mask & train_mask\n",
    "        veg_test_mask = veg_mask & test_mask\n",
    "\n",
    "        for col in numeric_cols:\n",
    "            if col in df.columns and df[col].isnull().any():\n",
    "                # Train: fill with train median\n",
    "                train_median = df.loc[veg_train_mask, col].median()\n",
    "                df.loc[veg_train_mask, col] = df.loc[veg_train_mask, col].fillna(train_median)\n",
    "\n",
    "                # Test: fill with train median (not test median!)\n",
    "                df.loc[veg_test_mask, col] = df.loc[veg_test_mask, col].fillna(train_median)\n",
    "\n",
    "    # 3. Handle categorical columns\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    categorical_cols = [col for col in categorical_cols if col not in ['Week_ID', 'Vegetable_Name']]\n",
    "\n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns and df[col].isnull().any():\n",
    "            # Use mode from training data\n",
    "            train_mode = df.loc[train_mask, col].mode()\n",
    "            mode_value = train_mode[0] if not train_mode.empty else 'Unknown'\n",
    "            df[col] = df[col].fillna(mode_value)\n",
    "\n",
    "    print(f\"\\nMissing values after handling: {df.isnull().sum().sum()}\")\n",
    "\n",
    "    # 4. Remove early weeks with insufficient history for lag features\n",
    "    # Keep only weeks where we have enough history for the longest lag (52 weeks)\n",
    "    print(f\"\\nRemoving early weeks with insufficient history...\")\n",
    "    initial_count = len(df)\n",
    "\n",
    "    # For each vegetable, remove first 52 weeks (or keep them with NaN for short lags)\n",
    "    # Actually, we already forward-filled, so we can keep them\n",
    "\n",
    "    print(f\"Kept {len(df)} of {initial_count} rows\")\n",
    "\n",
    "    return df\n",
    "\n",
    "weekly_df = handle_missing_values_robust(weekly_df)"
   ],
   "id": "d3c47b2ecaedcba4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values before handling:\n",
      "Columns with missing values: 42\n",
      "\n",
      "Missing values after handling: 0\n",
      "\n",
      "Removing early weeks with insufficient history...\n",
      "Kept 5034 of 5034 rows\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T17:36:49.800195800Z",
     "start_time": "2026-01-31T17:36:49.742906600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_targets_with_split(weekly_df, forecast_horizons=[1, 4]):\n",
    "    \"\"\"\n",
    "    Create targets and prepare for time-series cross-validation\n",
    "    \"\"\"\n",
    "    df = weekly_df.copy()\n",
    "\n",
    "    # Sort chronologically\n",
    "    df = df.sort_values(['Vegetable_Name', 'Week_Start'])\n",
    "\n",
    "    # Create target variables\n",
    "    print(f\"\\nCreating forecast targets for horizons: {forecast_horizons} weeks\")\n",
    "\n",
    "    for horizon in forecast_horizons:\n",
    "        for veg_name in df['Vegetable_Name'].unique():\n",
    "            veg_mask = df['Vegetable_Name'] == veg_name\n",
    "\n",
    "            # Price target\n",
    "            df.loc[veg_mask, f'Target_{horizon}w_Price'] = df.loc[veg_mask, 'Avg_Weekly_Price'].shift(-horizon)\n",
    "\n",
    "            # Arrival target\n",
    "            df.loc[veg_mask, f'Target_{horizon}w_Arrival'] = df.loc[veg_mask, 'Weekly_Arrival_Sum'].shift(-horizon)\n",
    "\n",
    "            # Sales target\n",
    "            df.loc[veg_mask, f'Target_{horizon}w_Sales'] = df.loc[veg_mask, 'Weekly_Sales_Sum'].shift(-horizon)\n",
    "\n",
    "    # Remove rows without future targets (last few weeks of each vegetable)\n",
    "    initial_len = len(df)\n",
    "    df = df.dropna(subset=['Target_1w_Price', 'Target_1w_Arrival'])\n",
    "    removed = initial_len - len(df)\n",
    "\n",
    "    print(f\"Removed {removed} rows without future targets\")\n",
    "    print(f\"Final dataset shape: {df.shape}\")\n",
    "\n",
    "    # Create time index for each vegetable\n",
    "    df['Time_Index'] = df.groupby('Vegetable_Name').cumcount()\n",
    "\n",
    "    return df\n",
    "\n",
    "weekly_df = create_targets_with_split(weekly_df)"
   ],
   "id": "b8df7ffe0d3994a3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating forecast targets for horizons: [1, 4] weeks\n",
      "Removed 6 rows without future targets\n",
      "Final dataset shape: (5028, 95)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T17:36:49.835315400Z",
     "start_time": "2026-01-31T17:36:49.802200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def select_final_features(weekly_df):\n",
    "    \"\"\"\n",
    "    Select final features for modeling, removing leakage-prone columns\n",
    "    \"\"\"\n",
    "    df = weekly_df.copy()\n",
    "\n",
    "    print(f\"Initial columns: {len(df.columns)}\")\n",
    "\n",
    "    # List of columns to REMOVE (leakage-prone, IDs, redundant)\n",
    "    drop_cols = [\n",
    "        # Identifiers and dates (keep for reference but not for modeling)\n",
    "        'Week_ID', 'Week_Start', 'MonthYear', 'Month_Vegetable',\n",
    "\n",
    "        # Future targets (except the main ones we're predicting)\n",
    "        'Target_1w_Sales', 'Target_4w_Sales', 'Target_4w_Arrival',\n",
    "        'Target_NextWeek_Arrival', 'Target_NextWeek_Sales',  # Legacy names\n",
    "\n",
    "        # Raw columns that are redundant with derived features\n",
    "        'Min_Daily_Arrival', 'Max_Daily_Arrival',\n",
    "        'Min_Daily_Sales', 'Max_Daily_Sales',\n",
    "        'Min_Weekly_Price', 'Max_Weekly_Price',\n",
    "\n",
    "        # Train-only statistics (leakage risk if used incorrectly)\n",
    "        'Veg_Train_Avg_Arrival', 'Veg_Train_Std_Arrival',\n",
    "        'Veg_Train_Avg_Price', 'Veg_Train_Std_Price', 'Veg_Train_Avg_Sales',\n",
    "        'MonthYear_Train_Avg_Arrival', 'MonthYear_Train_Avg_Price',\n",
    "\n",
    "        # Highly correlated with target (perfect predictors)\n",
    "        'Avg_Weekly_Price',  # Too correlated with target\n",
    "        'Weekly_Arrival_Sum',  # Use lagged versions instead\n",
    "        'Weekly_Sales_Sum',  # Use lagged versions instead\n",
    "\n",
    "        # Alternative versions (keep best one)\n",
    "        'Price_Arrival_Elasticity',  # Keep log version\n",
    "        'WeekOfYear',  # Keep sin/cos versions\n",
    "        'Month',  # Keep sin/cos versions\n",
    "    ]\n",
    "\n",
    "    # Only drop columns that exist\n",
    "    drop_cols = [col for col in drop_cols if col in df.columns]\n",
    "\n",
    "    print(f\"\\nDropping {len(drop_cols)} columns:\")\n",
    "    for col in drop_cols[:20]:  # Show first 20\n",
    "        print(f\"  - {col}\")\n",
    "    if len(drop_cols) > 20:\n",
    "        print(f\"  ... and {len(drop_cols) - 20} more\")\n",
    "\n",
    "    df_reduced = df.drop(columns=drop_cols)\n",
    "\n",
    "    # Keep essential reference columns separately\n",
    "    reference_cols = ['Vegetable_Name', 'Week_Start', 'ISO_Year', 'ISO_Week', 'Is_Train', 'Time_Index']\n",
    "    reference_df = df[reference_cols].copy()\n",
    "\n",
    "    # Separate features and targets\n",
    "    feature_cols = [col for col in df_reduced.columns\n",
    "                    if not col.startswith('Target_')\n",
    "                    and col not in reference_cols]\n",
    "\n",
    "    target_cols = [col for col in df_reduced.columns if col.startswith('Target_')]\n",
    "\n",
    "    print(f\"\\nFinal feature selection:\")\n",
    "    print(f\"  - Reference columns: {len(reference_cols)}\")\n",
    "    print(f\"  - Feature columns: {len(feature_cols)}\")\n",
    "    print(f\"  - Target columns: {len(target_cols)}\")\n",
    "    print(f\"  - Total columns kept: {len(reference_cols) + len(feature_cols) + len(target_cols)}\")\n",
    "\n",
    "    # Create feature categories for documentation\n",
    "    feature_categories = {\n",
    "        'Lag Features': [col for col in feature_cols if 'lag' in col],\n",
    "        'Moving Averages': [col for col in feature_cols if 'MA_' in col],\n",
    "        'Seasonal Features': [col for col in feature_cols if 'sin' in col or 'cos' in col or 'Season' in col],\n",
    "        'Volatility Features': [col for col in feature_cols if 'Volatility' in col or 'Std_' in col],\n",
    "        'Ratio Features': [col for col in feature_cols if 'Ratio' in col or 'Rate' in col],\n",
    "        'Trend Features': [col for col in feature_cols if 'Trend' in col],\n",
    "        'Deviation Features': [col for col in feature_cols if 'Dev_' in col or 'Z_Score' in col],\n",
    "        'Interaction Features': [col for col in feature_cols if 'Interaction' in col],\n",
    "    }\n",
    "\n",
    "    print(f\"\\nFeature categories:\")\n",
    "    for category, features in feature_categories.items():\n",
    "        if features:\n",
    "            print(f\"  {category}: {len(features)} features\")\n",
    "\n",
    "    return df, feature_cols, target_cols, reference_df\n",
    "\n",
    "final_df, feature_cols, target_cols, reference_df = select_final_features(weekly_df)"
   ],
   "id": "229a92b15c8dae57",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial columns: 96\n",
      "\n",
      "Dropping 24 columns:\n",
      "  - Week_ID\n",
      "  - Week_Start\n",
      "  - Target_1w_Sales\n",
      "  - Target_4w_Sales\n",
      "  - Target_4w_Arrival\n",
      "  - Min_Daily_Arrival\n",
      "  - Max_Daily_Arrival\n",
      "  - Min_Daily_Sales\n",
      "  - Max_Daily_Sales\n",
      "  - Min_Weekly_Price\n",
      "  - Max_Weekly_Price\n",
      "  - Veg_Train_Avg_Arrival\n",
      "  - Veg_Train_Std_Arrival\n",
      "  - Veg_Train_Avg_Price\n",
      "  - Veg_Train_Std_Price\n",
      "  - Veg_Train_Avg_Sales\n",
      "  - MonthYear_Train_Avg_Arrival\n",
      "  - MonthYear_Train_Avg_Price\n",
      "  - Avg_Weekly_Price\n",
      "  - Weekly_Arrival_Sum\n",
      "  ... and 4 more\n",
      "\n",
      "Final feature selection:\n",
      "  - Reference columns: 6\n",
      "  - Feature columns: 64\n",
      "  - Target columns: 3\n",
      "  - Total columns kept: 73\n",
      "\n",
      "Feature categories:\n",
      "  Lag Features: 21 features\n",
      "  Moving Averages: 8 features\n",
      "  Seasonal Features: 7 features\n",
      "  Volatility Features: 9 features\n",
      "  Ratio Features: 2 features\n",
      "  Trend Features: 2 features\n",
      "  Deviation Features: 4 features\n",
      "  Interaction Features: 2 features\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-31T17:36:50.697299800Z",
     "start_time": "2026-01-31T17:36:49.838338400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_final_datasets(final_df, feature_cols, target_cols, reference_df, processed_dir):\n",
    "    \"\"\"\n",
    "    Save processed datasets with clear separation\n",
    "    \"\"\"\n",
    "    # 1. Save full dataset\n",
    "    full_path = os.path.join(processed_dir, \"processed_weekly_full.csv\")\n",
    "    final_df.to_csv(full_path, index=False)\n",
    "\n",
    "    # 2. Save modeling-ready dataset (features + targets)\n",
    "    model_cols = feature_cols + target_cols\n",
    "    model_df = final_df[model_cols].copy()\n",
    "    model_path = os.path.join(processed_dir, \"processed_weekly_modeling.csv\")\n",
    "    model_df.to_csv(model_path, index=False)\n",
    "\n",
    "    # 3. Save reference data separately\n",
    "    ref_path = os.path.join(processed_dir, \"processed_weekly_reference.csv\")\n",
    "    reference_df.to_csv(ref_path, index=False)\n",
    "\n",
    "    # 4. Save feature list\n",
    "    feature_list_path = os.path.join(processed_dir, \"feature_list.txt\")\n",
    "    with open(feature_list_path, 'w') as f:\n",
    "        f.write(\"FINAL FEATURE LIST FOR MODELING\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "\n",
    "        f.write(f\"Total features: {len(feature_cols)}\\n\")\n",
    "        f.write(f\"Total targets: {len(target_cols)}\\n\\n\")\n",
    "\n",
    "        f.write(\"FEATURE CATEGORIES:\\n\")\n",
    "        f.write(\"-\" * 80 + \"\\n\\n\")\n",
    "\n",
    "        categories = {\n",
    "            'Lag Features': [col for col in feature_cols if 'lag' in col],\n",
    "            'Moving Averages': [col for col in feature_cols if 'MA_' in col],\n",
    "            'Seasonal': [col for col in feature_cols if 'sin' in col or 'cos' in col],\n",
    "            'Statistical': [col for col in feature_cols if 'Volatility' in col or 'Trend' in col or 'Ratio' in col],\n",
    "            'Encoded': [col for col in feature_cols if 'Encoded' in col],\n",
    "            'Other': [col for col in feature_cols if all(keyword not in col for keyword in\n",
    "                     ['lag', 'MA_', 'sin', 'cos', 'Volatility', 'Trend', 'Ratio', 'Encoded'])]\n",
    "        }\n",
    "\n",
    "        for category, features in categories.items():\n",
    "            if features:\n",
    "                f.write(f\"\\n{category.upper()} ({len(features)}):\\n\")\n",
    "                for feat in sorted(features):\n",
    "                    f.write(f\"  {feat}\\n\")\n",
    "\n",
    "        f.write(\"\\n\\nTARGET VARIABLES:\\n\")\n",
    "        f.write(\"-\" * 80 + \"\\n\")\n",
    "        for target in sorted(target_cols):\n",
    "            f.write(f\"{target}\\n\")\n",
    "\n",
    "        f.write(\"\\n\\nDATA SPLIT:\\n\")\n",
    "        f.write(\"-\" * 80 + \"\\n\")\n",
    "        train_count = reference_df['Is_Train'].sum()\n",
    "        test_count = (~reference_df['Is_Train']).sum()\n",
    "        f.write(f\"Training samples: {train_count} ({train_count/len(reference_df)*100:.1f}%)\\n\")\n",
    "        f.write(f\"Testing samples: {test_count} ({test_count/len(reference_df)*100:.1f}%)\\n\")\n",
    "        f.write(f\"Split date: 2023-01-01\\n\")\n",
    "\n",
    "    print(f\"\\nðŸ’¾ FILES SAVED:\")\n",
    "    print(f\"  1. Full dataset: {full_path}\")\n",
    "    print(f\"  2. Modeling dataset: {model_path}\")\n",
    "    print(f\"  3. Reference data: {ref_path}\")\n",
    "    print(f\"  4. Feature list: {feature_list_path}\")\n",
    "\n",
    "    return full_path, model_path\n",
    "\n",
    "full_path, model_path = save_final_datasets(final_df, feature_cols, target_cols, reference_df, processed_dir)"
   ],
   "id": "9f734752de64cb20",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ’¾ FILES SAVED:\n",
      "  1. Full dataset: ../data/processed/processed_weekly_full.csv\n",
      "  2. Modeling dataset: ../data/processed/processed_weekly_modeling.csv\n",
      "  3. Reference data: ../data/processed/processed_weekly_reference.csv\n",
      "  4. Feature list: ../data/processed/feature_list.txt\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
